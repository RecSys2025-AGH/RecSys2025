{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full embeddings shape: (1000000, 1280)\n",
            "Incorrect embeddings shape: (5939, 1280)\n",
            "Data type: PROPENSITY SKU predictions\n"
          ]
        }
      ],
      "source": [
        "# Load embeddings for propensity SKU analysis\n",
        "full_embeddings = np.load(\"solutions/best/embeddings.npy\")\n",
        "incorrect_embeddings = np.load(\"incorrect_embeddings/incorrect_embeddings_propensity_sku.npy\")\n",
        "\n",
        "print(f\"Full embeddings shape: {full_embeddings.shape}\")\n",
        "print(f\"Incorrect embeddings shape: {incorrect_embeddings.shape}\")\n",
        "print(f\"Data type: PROPENSITY SKU predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define embedding extraction functions\n",
        "indexes = {\n",
        "    \"add_to_cart\": 0,\n",
        "    \"remove_from_cart\": 256,\n",
        "    \"product_buys\": 512,\n",
        "    \"search_query\": 768,\n",
        "    \"page_visits\": 1024,\n",
        "}\n",
        "\n",
        "def get_embedding(action):\n",
        "    return full_embeddings[:, indexes[action]:indexes[action] + 256]\n",
        "\n",
        "def get_incorrect_embedding(action):\n",
        "    return incorrect_embeddings[:, indexes[action]:indexes[action] + 256]\n",
        "\n",
        "def value_counts(x):\n",
        "    unique, counts = np.unique(x, return_counts=True)\n",
        "    sorted_indices = np.argsort(-counts)\n",
        "    unique = unique[sorted_indices]\n",
        "    counts = counts[sorted_indices]\n",
        "    print(np.asarray((unique, counts)).T)\n",
        "\n",
        "def count_zero_vectors(arr):\n",
        "    zero_rows = np.all(arr == 0, axis=1)\n",
        "    return np.sum(zero_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting comprehensive PROPENSITY SKU analysis across all actions...\n",
            "\n",
            "============================================================\n",
            "ANALYZING ACTION: ADD_TO_CART (PROPENSITY SKU)\n",
            "============================================================\n",
            "=== BASIC STATISTICS ===\n",
            "Correct embeddings shape: (1000000, 256)\n",
            "Incorrect embeddings shape: (5939, 256)\n",
            "Zero vectors in correct: 387456\n",
            "Zero vectors in incorrect: 1022\n",
            "Non-zero correct embeddings: (612544, 256)\n",
            "Zero vector proportion - Correct: 0.3875, Incorrect: 0.1721\n",
            "\n",
            "=== EMBEDDING MAGNITUDES ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:171: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean norm: 14.1094, Std: inf\n",
            "Incorrect embeddings - Mean norm: 9.9501, Std: 4.9345\n",
            "\n",
            "=== STATISTICAL COMPARISON ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:205: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean: -0.0011, Std: inf\n",
            "Incorrect embeddings - Mean: -0.0010, Std: 0.6942\n",
            "\n",
            "=== SIMILARITY ANALYSIS ===\n",
            "Correct intra-group similarity - Mean: 0.5732, Std: 0.1400\n",
            "Incorrect intra-group similarity - Mean: 0.3345, Std: 0.2350\n",
            "Cross-group similarity - Mean: 0.3968, Std: 0.2114\n",
            "\n",
            "=== STATISTICAL TESTS ===\n",
            "KS test - Statistic: 0.0865, p-value: 0.00e+00\n",
            "Mann-Whitney U test on norms - Statistic: 20523443, p-value: 0.00e+00\n",
            "\n",
            "============================================================\n",
            "ANALYZING ACTION: REMOVE_FROM_CART (PROPENSITY SKU)\n",
            "============================================================\n",
            "=== BASIC STATISTICS ===\n",
            "Correct embeddings shape: (1000000, 256)\n",
            "Incorrect embeddings shape: (5939, 256)\n",
            "Zero vectors in correct: 675606\n",
            "Zero vectors in incorrect: 2431\n",
            "Non-zero correct embeddings: (324394, 256)\n",
            "Zero vector proportion - Correct: 0.6756, Incorrect: 0.4093\n",
            "\n",
            "=== EMBEDDING MAGNITUDES ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:171: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean norm: 14.0547, Std: inf\n",
            "Incorrect embeddings - Mean norm: 7.2641, Std: 6.3012\n",
            "\n",
            "=== STATISTICAL COMPARISON ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:205: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean: -0.0016, Std: inf\n",
            "Incorrect embeddings - Mean: -0.0013, Std: 0.6010\n",
            "\n",
            "=== SIMILARITY ANALYSIS ===\n",
            "Correct intra-group similarity - Mean: 0.4504, Std: 0.1172\n",
            "Incorrect intra-group similarity - Mean: 0.1427, Std: 0.2034\n",
            "Cross-group similarity - Mean: 0.2406, Std: 0.2173\n",
            "\n",
            "=== STATISTICAL TESTS ===\n",
            "KS test - Statistic: 0.2060, p-value: 0.00e+00\n",
            "Mann-Whitney U test on norms - Statistic: 21111784, p-value: 0.00e+00\n",
            "\n",
            "============================================================\n",
            "ANALYZING ACTION: PRODUCT_BUYS (PROPENSITY SKU)\n",
            "============================================================\n",
            "=== BASIC STATISTICS ===\n",
            "Correct embeddings shape: (1000000, 256)\n",
            "Incorrect embeddings shape: (5939, 256)\n",
            "Zero vectors in correct: 492448\n",
            "Zero vectors in incorrect: 69\n",
            "Non-zero correct embeddings: (507552, 256)\n",
            "Zero vector proportion - Correct: 0.4924, Incorrect: 0.0116\n",
            "\n",
            "=== EMBEDDING MAGNITUDES ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:171: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean norm: 14.8125, Std: inf\n",
            "Incorrect embeddings - Mean norm: 12.7140, Std: 2.5182\n",
            "\n",
            "=== STATISTICAL COMPARISON ===\n",
            "Correct embeddings - Mean: -0.0027, Std: inf\n",
            "Incorrect embeddings - Mean: -0.0032, Std: 0.8101\n",
            "\n",
            "=== SIMILARITY ANALYSIS ===\n",
            "Correct intra-group similarity - Mean: 0.4733, Std: 0.1347\n",
            "Incorrect intra-group similarity - Mean: 0.4089, Std: 0.1314\n",
            "Cross-group similarity - Mean: 0.3946, Std: 0.1235\n",
            "\n",
            "=== STATISTICAL TESTS ===\n",
            "KS test - Statistic: 0.0384, p-value: 0.00e+00\n",
            "Mann-Whitney U test on norms - Statistic: 20533537, p-value: 0.00e+00\n",
            "\n",
            "============================================================\n",
            "ANALYZING ACTION: SEARCH_QUERY (PROPENSITY SKU)\n",
            "============================================================\n",
            "=== BASIC STATISTICS ===\n",
            "Correct embeddings shape: (1000000, 256)\n",
            "Incorrect embeddings shape: (5939, 256)\n",
            "Zero vectors in correct: 676832\n",
            "Zero vectors in incorrect: 2311\n",
            "Non-zero correct embeddings: (323168, 256)\n",
            "Zero vector proportion - Correct: 0.6768, Incorrect: 0.3891\n",
            "\n",
            "=== EMBEDDING MAGNITUDES ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:171: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean norm: 12.2031, Std: inf\n",
            "Incorrect embeddings - Mean norm: 7.4928, Std: 6.1923\n",
            "\n",
            "=== STATISTICAL COMPARISON ===\n",
            "Correct embeddings - Mean: 0.0029, Std: inf\n",
            "Incorrect embeddings - Mean: 0.0014, Std: 0.6075\n",
            "\n",
            "=== SIMILARITY ANALYSIS ===\n",
            "Correct intra-group similarity - Mean: 0.4496, Std: 0.1696\n",
            "Incorrect intra-group similarity - Mean: 0.1873, Std: 0.2950\n",
            "Cross-group similarity - Mean: 0.2552, Std: 0.2656\n",
            "\n",
            "=== STATISTICAL TESTS ===\n",
            "KS test - Statistic: 0.1953, p-value: 0.00e+00\n",
            "Mann-Whitney U test on norms - Statistic: 17258687, p-value: 3.98e-240\n",
            "\n",
            "============================================================\n",
            "ANALYZING ACTION: PAGE_VISITS (PROPENSITY SKU)\n",
            "============================================================\n",
            "=== BASIC STATISTICS ===\n",
            "Correct embeddings shape: (1000000, 256)\n",
            "Incorrect embeddings shape: (5939, 256)\n",
            "Zero vectors in correct: 173519\n",
            "Zero vectors in incorrect: 815\n",
            "Non-zero correct embeddings: (826481, 256)\n",
            "Zero vector proportion - Correct: 0.1735, Incorrect: 0.1372\n",
            "\n",
            "=== EMBEDDING MAGNITUDES ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adam/python-projects/recsys25/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:171: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct embeddings - Mean norm: 10.9766, Std: inf\n",
            "Incorrect embeddings - Mean norm: 9.2495, Std: 4.0922\n",
            "\n",
            "=== STATISTICAL COMPARISON ===\n",
            "Correct embeddings - Mean: 0.0018, Std: inf\n",
            "Incorrect embeddings - Mean: 0.0018, Std: 0.6321\n",
            "\n",
            "=== SIMILARITY ANALYSIS ===\n",
            "Correct intra-group similarity - Mean: 0.5337, Std: 0.1307\n",
            "Incorrect intra-group similarity - Mean: 0.4485, Std: 0.2937\n",
            "Cross-group similarity - Mean: 0.4387, Std: 0.2181\n",
            "\n",
            "=== STATISTICAL TESTS ===\n",
            "KS test - Statistic: 0.0703, p-value: 0.00e+00\n",
            "Mann-Whitney U test on norms - Statistic: 15256561, p-value: 2.52e-81\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive analysis across all action types for PROPENSITY SKU\n",
        "actions = [\"add_to_cart\", \"remove_from_cart\", \"product_buys\", \"search_query\", \"page_visits\"]\n",
        "\n",
        "def analyze_action_propensity_sku(action_name):\n",
        "    \"\"\"Comprehensive analysis for a single action type - PROPENSITY SKU focus\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANALYZING ACTION: {action_name.upper()} (PROPENSITY SKU)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Get embeddings for this action\n",
        "    correct_emb = get_embedding(action_name)\n",
        "    incorrect_emb = get_incorrect_embedding(action_name)\n",
        "    \n",
        "    print(f\"=== BASIC STATISTICS ===\")\n",
        "    print(f\"Correct embeddings shape: {correct_emb.shape}\")\n",
        "    print(f\"Incorrect embeddings shape: {incorrect_emb.shape}\")\n",
        "    print(f\"Zero vectors in correct: {count_zero_vectors(correct_emb)}\")\n",
        "    print(f\"Zero vectors in incorrect: {count_zero_vectors(incorrect_emb)}\")\n",
        "    \n",
        "    # Remove zero vectors for fair comparison\n",
        "    non_zero_correct = correct_emb[~np.all(correct_emb == 0, axis=1)]\n",
        "    zero_prop_correct = count_zero_vectors(correct_emb) / len(correct_emb)\n",
        "    zero_prop_incorrect = count_zero_vectors(incorrect_emb) / len(incorrect_emb)\n",
        "    \n",
        "    print(f\"Non-zero correct embeddings: {non_zero_correct.shape}\")\n",
        "    print(f\"Zero vector proportion - Correct: {zero_prop_correct:.4f}, Incorrect: {zero_prop_incorrect:.4f}\")\n",
        "    \n",
        "    if len(non_zero_correct) == 0:\n",
        "        print(\"WARNING: No non-zero correct embeddings found!\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\n=== EMBEDDING MAGNITUDES ===\")\n",
        "    correct_norms = np.linalg.norm(non_zero_correct, axis=1)\n",
        "    incorrect_norms = np.linalg.norm(incorrect_emb, axis=1)\n",
        "    \n",
        "    print(f\"Correct embeddings - Mean norm: {np.mean(correct_norms):.4f}, Std: {np.std(correct_norms):.4f}\")\n",
        "    print(f\"Incorrect embeddings - Mean norm: {np.mean(incorrect_norms):.4f}, Std: {np.std(incorrect_norms):.4f}\")\n",
        "    \n",
        "    print(f\"\\n=== STATISTICAL COMPARISON ===\")\n",
        "    print(f\"Correct embeddings - Mean: {np.mean(non_zero_correct):.4f}, Std: {np.std(non_zero_correct):.4f}\")\n",
        "    print(f\"Incorrect embeddings - Mean: {np.mean(incorrect_emb):.4f}, Std: {np.std(incorrect_emb):.4f}\")\n",
        "    \n",
        "    # Sample for detailed analysis\n",
        "    n_samples = min(5000, len(non_zero_correct), len(incorrect_emb))\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    correct_sample_idx = np.random.choice(len(non_zero_correct), n_samples, replace=False)\n",
        "    incorrect_sample_idx = np.random.choice(len(incorrect_emb), n_samples, replace=False)\n",
        "    \n",
        "    correct_sample = non_zero_correct[correct_sample_idx]\n",
        "    incorrect_sample = incorrect_emb[incorrect_sample_idx]\n",
        "    \n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== SIMILARITY ANALYSIS ===\")\n",
        "    n_sim_samples = min(500, len(correct_sample), len(incorrect_sample))\n",
        "    \n",
        "    correct_similarities = cosine_similarity(correct_sample[:n_sim_samples])\n",
        "    incorrect_similarities = cosine_similarity(incorrect_sample[:n_sim_samples])\n",
        "    cross_similarities = cosine_similarity(correct_sample[:n_sim_samples], incorrect_sample[:n_sim_samples])\n",
        "    \n",
        "    def get_upper_triangle(matrix):\n",
        "        return matrix[np.triu_indices_from(matrix, k=1)]\n",
        "    \n",
        "    correct_sim_values = get_upper_triangle(correct_similarities)\n",
        "    incorrect_sim_values = get_upper_triangle(incorrect_similarities)\n",
        "    cross_sim_values = cross_similarities.flatten()\n",
        "    \n",
        "    print(f\"Correct intra-group similarity - Mean: {np.mean(correct_sim_values):.4f}, Std: {np.std(correct_sim_values):.4f}\")\n",
        "    print(f\"Incorrect intra-group similarity - Mean: {np.mean(incorrect_sim_values):.4f}, Std: {np.std(incorrect_sim_values):.4f}\")\n",
        "    print(f\"Cross-group similarity - Mean: {np.mean(cross_sim_values):.4f}, Std: {np.std(cross_sim_values):.4f}\")\n",
        "    \n",
        "    # Statistical tests\n",
        "    print(f\"\\n=== STATISTICAL TESTS ===\")\n",
        "    ks_stat, ks_pvalue = stats.ks_2samp(correct_sample.flatten(), incorrect_sample.flatten())\n",
        "    print(f\"KS test - Statistic: {ks_stat:.4f}, p-value: {ks_pvalue:.2e}\")\n",
        "    \n",
        "    correct_sample_norms = np.linalg.norm(correct_sample, axis=1)\n",
        "    incorrect_sample_norms = np.linalg.norm(incorrect_sample, axis=1)\n",
        "    mw_stat, mw_pvalue = stats.mannwhitneyu(correct_sample_norms, incorrect_sample_norms)\n",
        "    print(f\"Mann-Whitney U test on norms - Statistic: {mw_stat:.0f}, p-value: {mw_pvalue:.2e}\")\n",
        "    \n",
        "    return {\n",
        "        'action': action_name,\n",
        "        'zero_prop_correct': zero_prop_correct,\n",
        "        'zero_prop_incorrect': zero_prop_incorrect,\n",
        "        'correct_norm_mean': np.mean(correct_norms),\n",
        "        'incorrect_norm_mean': np.mean(incorrect_norms),\n",
        "        'correct_sim_mean': np.mean(correct_sim_values),\n",
        "        'incorrect_sim_mean': np.mean(incorrect_sim_values),\n",
        "        'cross_sim_mean': np.mean(cross_sim_values),\n",
        "        'ks_stat': ks_stat,\n",
        "        'ks_pvalue': ks_pvalue\n",
        "    }\n",
        "\n",
        "# Run analysis for all actions\n",
        "print(\"Starting comprehensive PROPENSITY SKU analysis across all actions...\")\n",
        "results = []\n",
        "for action in actions:\n",
        "    result = analyze_action_propensity_sku(action)\n",
        "    if result is not None:\n",
        "        results.append(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY COMPARISON ACROSS ALL ACTIONS - PROPENSITY SKU\n",
            "================================================================================\n",
            "          action  zero_prop_correct  zero_prop_incorrect  correct_norm_mean  incorrect_norm_mean  correct_sim_mean  incorrect_sim_mean  cross_sim_mean  ks_stat  ks_pvalue\n",
            "     add_to_cart             0.3875               0.1721            14.1094               9.9501            0.5732              0.3345          0.3968   0.0865     0.0000\n",
            "remove_from_cart             0.6756               0.4093            14.0547               7.2641            0.4504              0.1427          0.2406   0.2060     0.0000\n",
            "    product_buys             0.4924               0.0116            14.8125              12.7140            0.4733              0.4089          0.3946   0.0384     0.0000\n",
            "    search_query             0.6768               0.3891            12.2031               7.4928            0.4496              0.1873          0.2552   0.1953     0.0000\n",
            "     page_visits             0.1735               0.1372            10.9766               9.2495            0.5337              0.4485          0.4387   0.0703     0.0000\n",
            "\n",
            "=== KEY INSIGHTS ACROSS ACTIONS (PROPENSITY SKU) ===\n",
            "\n",
            "1. ACTIVITY PATTERNS (Zero Vector Proportions):\n",
            "   add_to_cart     - Correct: 38.7%, Incorrect: 17.2%\n",
            "   remove_from_cart - Correct: 67.6%, Incorrect: 40.9%\n",
            "   product_buys    - Correct: 49.2%, Incorrect: 1.2%\n",
            "   search_query    - Correct: 67.7%, Incorrect: 38.9%\n",
            "   page_visits     - Correct: 17.4%, Incorrect: 13.7%\n",
            "\n",
            "2. EMBEDDING MAGNITUDE DIFFERENCES:\n",
            "   add_to_cart     - Incorrect norms LOWER by 4.159\n",
            "   remove_from_cart - Incorrect norms LOWER by 6.791\n",
            "   product_buys    - Incorrect norms LOWER by 2.098\n",
            "   search_query    - Incorrect norms LOWER by 4.710\n",
            "   page_visits     - Incorrect norms LOWER by 1.727\n",
            "\n",
            "3. SIMILARITY PATTERNS:\n",
            "   add_to_cart     - Cross-group similarity: 0.397 (MODERATE)\n",
            "   remove_from_cart - Cross-group similarity: 0.241 (MODERATE)\n",
            "   product_buys    - Cross-group similarity: 0.395 (MODERATE)\n",
            "   search_query    - Cross-group similarity: 0.255 (MODERATE)\n",
            "   page_visits     - Cross-group similarity: 0.439 (MODERATE)\n",
            "\n",
            "4. STATISTICAL SIGNIFICANCE:\n",
            "   add_to_cart     - KS test: SIGNIFICANT (p=0.00e+00)\n",
            "   remove_from_cart - KS test: SIGNIFICANT (p=0.00e+00)\n",
            "   product_buys    - KS test: SIGNIFICANT (p=0.00e+00)\n",
            "   search_query    - KS test: SIGNIFICANT (p=0.00e+00)\n",
            "   page_visits     - KS test: SIGNIFICANT (p=0.00e+00)\n",
            "\n",
            "5. MOST PROBLEMATIC ACTIONS (ranked by issues):\n",
            "   add_to_cart     - Problem score: 3/3\n",
            "   product_buys    - Problem score: 3/3\n",
            "   page_visits     - Problem score: 3/3\n",
            "   remove_from_cart - Problem score: 2/3\n",
            "   search_query    - Problem score: 2/3\n"
          ]
        }
      ],
      "source": [
        "# Create summary comparison across all actions for PROPENSITY SKU\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"SUMMARY COMPARISON ACROSS ALL ACTIONS - PROPENSITY SKU\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    print(df.to_string(index=False, float_format='%.4f'))\n",
        "    \n",
        "    # Key insights\n",
        "    print(f\"\\n=== KEY INSIGHTS ACROSS ACTIONS (PROPENSITY SKU) ===\")\n",
        "    \n",
        "    print(\"\\n1. ACTIVITY PATTERNS (Zero Vector Proportions):\")\n",
        "    for _, row in df.iterrows():\n",
        "        print(f\"   {row['action']:15} - Correct: {row['zero_prop_correct']:.1%}, Incorrect: {row['zero_prop_incorrect']:.1%}\")\n",
        "    \n",
        "    print(\"\\n2. EMBEDDING MAGNITUDE DIFFERENCES:\")\n",
        "    for _, row in df.iterrows():\n",
        "        norm_diff = row['incorrect_norm_mean'] - row['correct_norm_mean']\n",
        "        direction = \"HIGHER\" if norm_diff > 0 else \"LOWER\"\n",
        "        print(f\"   {row['action']:15} - Incorrect norms {direction} by {abs(norm_diff):.3f}\")\n",
        "    \n",
        "    print(\"\\n3. SIMILARITY PATTERNS:\")\n",
        "    for _, row in df.iterrows():\n",
        "        cross_sim = row['cross_sim_mean']\n",
        "        status = \"HIGH\" if cross_sim > 0.5 else \"MODERATE\" if cross_sim > 0.2 else \"LOW\"\n",
        "        print(f\"   {row['action']:15} - Cross-group similarity: {cross_sim:.3f} ({status})\")\n",
        "    \n",
        "    print(\"\\n4. STATISTICAL SIGNIFICANCE:\")\n",
        "    for _, row in df.iterrows():\n",
        "        significance = \"SIGNIFICANT\" if row['ks_pvalue'] < 0.001 else \"NOT SIGNIFICANT\"\n",
        "        print(f\"   {row['action']:15} - KS test: {significance} (p={row['ks_pvalue']:.2e})\")\n",
        "        \n",
        "    # Find most problematic actions\n",
        "    print(\"\\n5. MOST PROBLEMATIC ACTIONS (ranked by issues):\")\n",
        "    df['problem_score'] = (\n",
        "        (df['zero_prop_incorrect'] < df['zero_prop_correct']) * 1 +  # Incorrect has fewer zeros\n",
        "        (df['cross_sim_mean'] > 0.3) * 1 +  # High cross-similarity\n",
        "        (df['ks_pvalue'] < 0.001) * 1  # Statistically different\n",
        "    )\n",
        "    problematic = df.sort_values('problem_score', ascending=False)\n",
        "    for _, row in problematic.iterrows():\n",
        "        print(f\"   {row['action']:15} - Problem score: {row['problem_score']}/3\")\n",
        "\n",
        "else:\n",
        "    print(\"No results to analyze.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL ANALYSIS AND RECOMMENDATIONS - PROPENSITY SKU\n",
            "================================================================================\n",
            "\n",
            "=== COMMON PATTERNS ACROSS ALL ACTIONS (PROPENSITY SKU) ===\n",
            "1. ACTIVITY BIAS (incorrect has fewer inactive users): ['add_to_cart', 'remove_from_cart', 'product_buys', 'search_query', 'page_visits']\n",
            "2. HIGH CROSS-SIMILARITY (>0.3): ['add_to_cart', 'product_buys', 'page_visits']\n",
            "3. SIGNIFICANT NORM DIFFERENCES (>0.1): ['add_to_cart', 'remove_from_cart', 'product_buys', 'search_query', 'page_visits']\n",
            "\n",
            "=== ROOT CAUSE ANALYSIS (PROPENSITY SKU) ===\n",
            "The model's propensity SKU predictions show:\n",
            "\n",
            "1. SKU-LEVEL PREDICTION CHALLENGES:\n",
            "   - Model struggles with fine-grained SKU-level predictions\n",
            "   - SKU preferences are highly individual and context-dependent\n",
            "   - User behavior patterns may not capture specific product affinities\n",
            "   - High SKU diversity makes prediction extremely challenging\n",
            "\n",
            "2. EMBEDDING GRANULARITY ISSUES:\n",
            "\n",
            "3. PRODUCT-SPECIFIC PATTERNS:\n",
            "   - 3/5 actions show poor SKU prediction separation\n",
            "   - User embeddings may not capture specific product preferences\n",
            "   - SKU-level distinctions may be too subtle for current embeddings\n",
            "\n",
            "=== RECOMMENDATIONS FOR PROPENSITY SKU MODELING ===\n",
            "1. PRODUCT-SPECIFIC FEATURES:\n",
            "   - Add detailed product attributes and descriptions\n",
            "   - Include product similarity and substitution patterns\n",
            "   - Consider product lifecycle and popularity features\n",
            "   - Add brand, price, and attribute preferences\n",
            "2. MODEL ARCHITECTURE:\n",
            "   - Implement hierarchical modeling (category -> brand -> SKU)\n",
            "   - Use product2vec or similar product embeddings\n",
            "   - Consider collaborative filtering for SKU-level predictions\n",
            "   - Add product attention mechanisms\n",
            "3. DATA REPRESENTATION:\n",
            "   - Improve SKU embedding quality with richer features\n",
            "   - Consider temporal SKU preferences and seasonality\n",
            "   - Add cross-SKU interaction patterns\n",
            "   - Include purchase context (occasion, seasonality)\n",
            "4. EVALUATION APPROACH:\n",
            "   - SKU-specific evaluation metrics\n",
            "   - Consider recommendation diversity vs precision trade-offs\n",
            "   - Evaluate long-tail SKU coverage\n",
            "   - Use hierarchical evaluation (category accuracy -> SKU accuracy)\n",
            "5. ALTERNATIVE APPROACHES:\n",
            "   - Consider ensemble with category-level predictions\n",
            "   - Implement cold-start strategies for new SKUs\n",
            "   - Use content-based filtering for sparse SKUs\n",
            "   - Consider popularity-based fallbacks\n"
          ]
        }
      ],
      "source": [
        "# Final analysis and recommendations for PROPENSITY SKU\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL ANALYSIS AND RECOMMENDATIONS - PROPENSITY SKU\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if results:\n",
        "    # Identify patterns across all actions\n",
        "    print(\"\\n=== COMMON PATTERNS ACROSS ALL ACTIONS (PROPENSITY SKU) ===\")\n",
        "    \n",
        "    activity_bias_actions = [r['action'] for r in results if r['zero_prop_incorrect'] < r['zero_prop_correct']]\n",
        "    high_cross_sim_actions = [r['action'] for r in results if r['cross_sim_mean'] > 0.3]\n",
        "    norm_difference_actions = [r['action'] for r in results if abs(r['incorrect_norm_mean'] - r['correct_norm_mean']) > 0.1]\n",
        "    \n",
        "    print(f\"1. ACTIVITY BIAS (incorrect has fewer inactive users): {activity_bias_actions}\")\n",
        "    print(f\"2. HIGH CROSS-SIMILARITY (>0.3): {high_cross_sim_actions}\")\n",
        "    print(f\"3. SIGNIFICANT NORM DIFFERENCES (>0.1): {norm_difference_actions}\")\n",
        "    \n",
        "    print(f\"\\n=== ROOT CAUSE ANALYSIS (PROPENSITY SKU) ===\")\n",
        "    print(\"The model's propensity SKU predictions show:\")\n",
        "    \n",
        "    print(\"\\n1. SKU-LEVEL PREDICTION CHALLENGES:\")\n",
        "    print(\"   - Model struggles with fine-grained SKU-level predictions\")\n",
        "    print(\"   - SKU preferences are highly individual and context-dependent\")\n",
        "    print(\"   - User behavior patterns may not capture specific product affinities\")\n",
        "    print(\"   - High SKU diversity makes prediction extremely challenging\")\n",
        "    \n",
        "    print(\"\\n2. EMBEDDING GRANULARITY ISSUES:\")\n",
        "    norm_higher_count = sum(1 for r in results if r['incorrect_norm_mean'] > r['correct_norm_mean'])\n",
        "    if norm_higher_count > len(results) / 2:\n",
        "        print(\"   - Incorrect SKU predictions have higher embedding magnitudes\")\n",
        "        print(\"   - Model may be overconfident about wrong specific products\")\n",
        "        print(\"   - SKU-level features may be too sparse or noisy\")\n",
        "    \n",
        "    print(\"\\n3. PRODUCT-SPECIFIC PATTERNS:\")\n",
        "    poor_separation_count = sum(1 for r in results if r['cross_sim_mean'] > 0.3)\n",
        "    if poor_separation_count > 0:\n",
        "        print(f\"   - {poor_separation_count}/{len(results)} actions show poor SKU prediction separation\")\n",
        "        print(\"   - User embeddings may not capture specific product preferences\")\n",
        "        print(\"   - SKU-level distinctions may be too subtle for current embeddings\")\n",
        "    \n",
        "    print(f\"\\n=== RECOMMENDATIONS FOR PROPENSITY SKU MODELING ===\")\n",
        "    print(\"1. PRODUCT-SPECIFIC FEATURES:\")\n",
        "    print(\"   - Add detailed product attributes and descriptions\")\n",
        "    print(\"   - Include product similarity and substitution patterns\")\n",
        "    print(\"   - Consider product lifecycle and popularity features\")\n",
        "    print(\"   - Add brand, price, and attribute preferences\")\n",
        "    \n",
        "    print(\"2. MODEL ARCHITECTURE:\")\n",
        "    print(\"   - Implement hierarchical modeling (category -> brand -> SKU)\")\n",
        "    print(\"   - Use product2vec or similar product embeddings\")\n",
        "    print(\"   - Consider collaborative filtering for SKU-level predictions\")\n",
        "    print(\"   - Add product attention mechanisms\")\n",
        "    \n",
        "    print(\"3. DATA REPRESENTATION:\")\n",
        "    print(\"   - Improve SKU embedding quality with richer features\")\n",
        "    print(\"   - Consider temporal SKU preferences and seasonality\")\n",
        "    print(\"   - Add cross-SKU interaction patterns\")\n",
        "    print(\"   - Include purchase context (occasion, seasonality)\")\n",
        "    \n",
        "    print(\"4. EVALUATION APPROACH:\")\n",
        "    print(\"   - SKU-specific evaluation metrics\")\n",
        "    print(\"   - Consider recommendation diversity vs precision trade-offs\")\n",
        "    print(\"   - Evaluate long-tail SKU coverage\")\n",
        "    print(\"   - Use hierarchical evaluation (category accuracy -> SKU accuracy)\")\n",
        "    \n",
        "    print(\"5. ALTERNATIVE APPROACHES:\")\n",
        "    print(\"   - Consider ensemble with category-level predictions\")\n",
        "    print(\"   - Implement cold-start strategies for new SKUs\")\n",
        "    print(\"   - Use content-based filtering for sparse SKUs\")\n",
        "    print(\"   - Consider popularity-based fallbacks\")\n",
        "\n",
        "else:\n",
        "    print(\"No results to analyze.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
