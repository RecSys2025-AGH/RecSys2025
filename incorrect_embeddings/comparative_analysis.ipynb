{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import stats\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all types of incorrect embeddings for comparative analysis\n",
        "full_embeddings = np.load(\"../solutions/best/embeddings.npy\")\n",
        "churn_embeddings = np.load(\"incorrect_embeddings_churn.npy\")\n",
        "category_embeddings = np.load(\"incorrect_embeddings_propensity_category.npy\")\n",
        "sku_embeddings = np.load(\"incorrect_embeddings_propensity_sku.npy\")\n",
        "\n",
        "print(\"=== COMPARATIVE ANALYSIS OF INCORRECT PREDICTIONS ===\")\n",
        "print(f\"Full embeddings shape: {full_embeddings.shape}\")\n",
        "print(f\"Churn incorrect embeddings shape: {churn_embeddings.shape}\")\n",
        "print(f\"Category incorrect embeddings shape: {category_embeddings.shape}\")\n",
        "print(f\"SKU incorrect embeddings shape: {sku_embeddings.shape}\")\n",
        "\n",
        "prediction_types = {\n",
        "    'churn': churn_embeddings,\n",
        "    'category': category_embeddings, \n",
        "    'sku': sku_embeddings\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "indexes = {\n",
        "    \"add_to_cart\": 0,\n",
        "    \"remove_from_cart\": 256,\n",
        "    \"product_buys\": 512,\n",
        "    \"search_query\": 768,\n",
        "    \"page_visits\": 1024,\n",
        "}\n",
        "\n",
        "def get_embedding(embeddings, action):\n",
        "    return embeddings[:, indexes[action]:indexes[action] + 256]\n",
        "\n",
        "def count_zero_vectors(arr):\n",
        "    zero_rows = np.all(arr == 0, axis=1)\n",
        "    return np.sum(zero_rows)\n",
        "\n",
        "def analyze_prediction_type(prediction_type, incorrect_embeddings, correct_embeddings):\n",
        "    \"\"\"Analyze a specific prediction type across all actions\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANALYZING {prediction_type.upper()} PREDICTIONS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    results = []\n",
        "    actions = [\"add_to_cart\", \"remove_from_cart\", \"product_buys\", \"search_query\", \"page_visits\"]\n",
        "    \n",
        "    for action in actions:\n",
        "        correct_emb = get_embedding(correct_embeddings, action)\n",
        "        incorrect_emb = get_embedding(incorrect_embeddings, action)\n",
        "        \n",
        "        # Basic statistics\n",
        "        zero_correct = count_zero_vectors(correct_emb) / len(correct_emb)\n",
        "        zero_incorrect = count_zero_vectors(incorrect_emb) / len(incorrect_emb)\n",
        "        \n",
        "        # Remove zero vectors for norm analysis\n",
        "        non_zero_correct = correct_emb[~np.all(correct_emb == 0, axis=1)]\n",
        "        if len(non_zero_correct) == 0:\n",
        "            continue\n",
        "            \n",
        "        correct_norms = np.linalg.norm(non_zero_correct, axis=1)\n",
        "        incorrect_norms = np.linalg.norm(incorrect_emb, axis=1)\n",
        "        \n",
        "        # Sample for similarity analysis\n",
        "        n_samples = min(1000, len(non_zero_correct), len(incorrect_emb))\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        if n_samples > 0:\n",
        "            correct_sample_idx = np.random.choice(len(non_zero_correct), n_samples, replace=False)\\n            incorrect_sample_idx = np.random.choice(len(incorrect_emb), n_samples, replace=False)\\n            \\n            correct_sample = non_zero_correct[correct_sample_idx]\\n            incorrect_sample = incorrect_emb[incorrect_sample_idx]\\n            \\n            # Cross-similarity\\n            cross_sim = cosine_similarity(correct_sample[:100], incorrect_sample[:100])\\n            \\n            results.append({\\n                'prediction_type': prediction_type,\\n                'action': action,\\n                'zero_prop_correct': zero_correct,\\n                'zero_prop_incorrect': zero_incorrect,\\n                'correct_norm_mean': np.mean(correct_norms),\\n                'incorrect_norm_mean': np.mean(incorrect_norms),\\n                'cross_sim_mean': np.mean(cross_sim),\\n                'norm_difference': np.mean(incorrect_norms) - np.mean(correct_norms)\\n            })\\n    \\n    return results\\n\\n# Analyze all prediction types\\nall_results = []\\nfor pred_type, incorrect_emb in prediction_types.items():\\n    results = analyze_prediction_type(pred_type, incorrect_emb, full_embeddings)\\n    all_results.extend(results)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Create comprehensive comparison\\n\",\n",
        "    \"df = pd.DataFrame(all_results)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
        "    \"print(\\\"COMPREHENSIVE COMPARISON ACROSS PREDICTION TYPES\\\")\\n\",\n",
        "    \"print(f\\\"{'='*80}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Create pivot tables for easy comparison\\n\",\n",
        "    \"pivot_zero_correct = df.pivot(index='action', columns='prediction_type', values='zero_prop_correct')\\n\",\n",
        "    \"pivot_zero_incorrect = df.pivot(index='action', columns='prediction_type', values='zero_prop_incorrect')\\n\",\n",
        "    \"pivot_norm_diff = df.pivot(index='action', columns='prediction_type', values='norm_difference')\\n\",\n",
        "    \"pivot_cross_sim = df.pivot(index='action', columns='prediction_type', values='cross_sim_mean')\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== ZERO VECTOR PROPORTIONS (CORRECT PREDICTIONS) ===\\\")\\n\",\n",
        "    \"print(pivot_zero_correct.round(4))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== ZERO VECTOR PROPORTIONS (INCORRECT PREDICTIONS) ===\\\")\\n\",\n",
        "    \"print(pivot_zero_incorrect.round(4))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== NORM DIFFERENCES (Incorrect - Correct) ===\\\")\\n\",\n",
        "    \"print(pivot_norm_diff.round(4))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== CROSS-GROUP SIMILARITIES ===\\\")\\n\",\n",
        "    \"print(pivot_cross_sim.round(4))\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Visualize comparisons across prediction types\\n\",\n",
        "    \"fig, axes = plt.subplots(2, 2, figsize=(15, 12))\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Plot 1: Zero vector proportions for incorrect predictions\\n\",\n",
        "    \"pivot_zero_incorrect.plot(kind='bar', ax=axes[0,0], alpha=0.8)\\n\",\n",
        "    \"axes[0,0].set_title('Zero Vector Proportions (Incorrect Predictions)')\\n\",\n",
        "    \"axes[0,0].set_ylabel('Proportion')\\n\",\n",
        "    \"axes[0,0].legend(title='Prediction Type')\\n\",\n",
        "    \"axes[0,0].tick_params(axis='x', rotation=45)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Plot 2: Norm differences\\n\",\n",
        "    \"pivot_norm_diff.plot(kind='bar', ax=axes[0,1], alpha=0.8)\\n\",\n",
        "    \"axes[0,1].set_title('Norm Differences (Incorrect - Correct)')\\n\",\n",
        "    \"axes[0,1].set_ylabel('Norm Difference')\\n\",\n",
        "    \"axes[0,1].legend(title='Prediction Type')\\n\",\n",
        "    \"axes[0,1].tick_params(axis='x', rotation=45)\\n\",\n",
        "    \"axes[0,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Plot 3: Cross-group similarities\\n\",\n",
        "    \"pivot_cross_sim.plot(kind='bar', ax=axes[1,0], alpha=0.8)\\n\",\n",
        "    \"axes[1,0].set_title('Cross-Group Similarities')\\n\",\n",
        "    \"axes[1,0].set_ylabel('Cosine Similarity')\\n\",\n",
        "    \"axes[1,0].legend(title='Prediction Type')\\n\",\n",
        "    \"axes[1,0].tick_params(axis='x', rotation=45)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Plot 4: Activity bias comparison\\n\",\n",
        "    \"activity_bias = pivot_zero_correct - pivot_zero_incorrect\\n\",\n",
        "    \"activity_bias.plot(kind='bar', ax=axes[1,1], alpha=0.8)\\n\",\n",
        "    \"axes[1,1].set_title('Activity Bias (Correct_zeros - Incorrect_zeros)')\\n\",\n",
        "    \"axes[1,1].set_ylabel('Proportion Difference')\\n\",\n",
        "    \"axes[1,1].legend(title='Prediction Type')\\n\",\n",
        "    \"axes[1,1].tick_params(axis='x', rotation=45)\\n\",\n",
        "    \"axes[1,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\\n\",\n",
        "    \"\\n\",\n",
        "    \"plt.tight_layout()\\n\",\n",
        "    \"plt.show()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Cross-prediction type similarity analysis\\n\",\n",
        "    \"print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
        "    \"print(\\\"CROSS-PREDICTION TYPE SIMILARITY ANALYSIS\\\")\\n\",\n",
        "    \"print(f\\\"{'='*80}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Compare similarities between different prediction types\\n\",\n",
        "    \"for action in [\\\"product_buys\\\", \\\"add_to_cart\\\", \\\"page_visits\\\"]:\\n\",\n",
        "    \"    print(f\\\"\\\\n=== {action.upper()} ACTION ===\\\")\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Get embeddings for this action across all prediction types\\n\",\n",
        "    \"    churn_emb = get_embedding(churn_embeddings, action)\\n\",\n",
        "    \"    category_emb = get_embedding(category_embeddings, action)\\n\",\n",
        "    \"    sku_emb = get_embedding(sku_embeddings, action)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Sample for analysis\\n\",\n",
        "    \"    n_samples = min(500, len(churn_emb), len(category_emb), len(sku_emb))\\n\",\n",
        "    \"    np.random.seed(42)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    indices = np.random.choice(len(churn_emb), n_samples, replace=False)\\n\",\n",
        "    \"    churn_sample = churn_emb[indices]\\n\",\n",
        "    \"    category_sample = category_emb[indices]\\n\",\n",
        "    \"    sku_sample = sku_emb[indices]\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Calculate cross-similarities\\n\",\n",
        "    \"    churn_category_sim = cosine_similarity(churn_sample, category_sample)\\n\",\n",
        "    \"    churn_sku_sim = cosine_similarity(churn_sample, sku_sample)\\n\",\n",
        "    \"    category_sku_sim = cosine_similarity(category_sample, sku_sample)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    print(f\\\"Churn vs Category similarity: {np.mean(churn_category_sim):.4f} ± {np.std(churn_category_sim):.4f}\\\")\\n\",\n",
        "    \"    print(f\\\"Churn vs SKU similarity: {np.mean(churn_sku_sim):.4f} ± {np.std(churn_sku_sim):.4f}\\\")\\n\",\n",
        "    \"    print(f\\\"Category vs SKU similarity: {np.mean(category_sku_sim):.4f} ± {np.std(category_sku_sim):.4f}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Final comparative insights and recommendations\\n\",\n",
        "    \"print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
        "    \"print(\\\"FINAL COMPARATIVE INSIGHTS AND RECOMMENDATIONS\\\")\\n\",\n",
        "    \"print(f\\\"{'='*80}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== KEY DIFFERENCES BETWEEN PREDICTION TYPES ===\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Analyze patterns across prediction types\\n\",\n",
        "    \"mean_metrics = df.groupby('prediction_type').agg({\\n\",\n",
        "    \"    'zero_prop_incorrect': 'mean',\\n\",\n",
        "    \"    'norm_difference': 'mean', \\n\",\n",
        "    \"    'cross_sim_mean': 'mean'\\n\",\n",
        "    \"}).round(4)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\nAVERAGE METRICS BY PREDICTION TYPE:\\\")\\n\",\n",
        "    \"print(mean_metrics)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== RANKING BY DIFFICULTY ===\\\")\\n\",\n",
        "    \"# Create difficulty score based on multiple factors\\n\",\n",
        "    \"difficulty_scores = df.groupby('prediction_type').agg({\\n\",\n",
        "    \"    'cross_sim_mean': 'mean',  # Higher = worse separation\\n\",\n",
        "    \"    'norm_difference': lambda x: np.mean(np.abs(x))  # Higher absolute = more different\\n\",\n",
        "    \"})\\n\",\n",
        "    \"\\n\",\n",
        "    \"difficulty_scores['difficulty_score'] = (\\n\",\n",
        "    \"    difficulty_scores['cross_sim_mean'] * 2 +  # Weight cross-similarity higher\\n\",\n",
        "    \"    (1 - difficulty_scores['norm_difference'])  # Lower norm diff = more difficult\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"difficulty_ranking = difficulty_scores.sort_values('difficulty_score', ascending=False)\\n\",\n",
        "    \"print(\\\"\\\\nPREDICTION DIFFICULTY RANKING (1=hardest):\\\")\\n\",\n",
        "    \"for i, (pred_type, _) in enumerate(difficulty_ranking.iterrows(), 1):\\n\",\n",
        "    \"    print(f\\\"{i}. {pred_type.upper()} - Score: {difficulty_ranking.loc[pred_type, 'difficulty_score']:.3f}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== SPECIFIC INSIGHTS BY PREDICTION TYPE ===\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n1. CHURN PREDICTIONS:\\\")\\n\",\n",
        "    \"churn_data = df[df['prediction_type'] == 'churn']\\n\",\n",
        "    \"print(f\\\"   - Average cross-similarity: {churn_data['cross_sim_mean'].mean():.3f}\\\")\\n\",\n",
        "    \"print(f\\\"   - Tends to predict for active users (fewer zero vectors)\\\")\\n\",\n",
        "    \"print(f\\\"   - Model challenge: Distinguishing active users who will churn vs stay\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n2. CATEGORY PREDICTIONS:\\\")\\n\",\n",
        "    \"category_data = df[df['prediction_type'] == 'category']\\n\",\n",
        "    \"print(f\\\"   - Average cross-similarity: {category_data['cross_sim_mean'].mean():.3f}\\\")\\n\",\n",
        "    \"print(f\\\"   - Challenge: User behavior doesn't strongly indicate category preferences\\\")\\n\",\n",
        "    \"print(f\\\"   - Model needs better category-specific features\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n3. SKU PREDICTIONS:\\\")\\n\",\n",
        "    \"sku_data = df[df['prediction_type'] == 'sku']\\n\",\n",
        "    \"print(f\\\"   - Average cross-similarity: {sku_data['cross_sim_mean'].mean():.3f}\\\")\\n\",\n",
        "    \"print(f\\\"   - Challenge: Extremely fine-grained predictions with high item diversity\\\")\\n\",\n",
        "    \"print(f\\\"   - Model needs hierarchical approach and product-specific features\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== UNIFIED RECOMMENDATIONS ===\\\")\\n\",\n",
        "    \"print(\\\"1. TASK-SPECIFIC ARCHITECTURES:\\\")\\n\",\n",
        "    \"print(\\\"   - Use different model architectures for different prediction granularities\\\")\\n\",\n",
        "    \"print(\\\"   - Churn: Focus on temporal patterns and engagement metrics\\\")\\n\",\n",
        "    \"print(\\\"   - Category: Add category hierarchy and cross-category patterns\\\")\\n\",\n",
        "    \"print(\\\"   - SKU: Implement hierarchical modeling with product embeddings\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n2. EMBEDDING IMPROVEMENTS:\\\")\\n\",\n",
        "    \"print(\\\"   - Current embeddings work better for coarse-grained tasks (churn)\\\")\\n\",\n",
        "    \"print(\\\"   - Need specialized embeddings for fine-grained tasks (SKU, category)\\\")\\n\",\n",
        "    \"print(\\\"   - Consider task-specific embedding spaces\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n3. EVALUATION STRATEGIES:\\\")\\n\",\n",
        "    \"print(\\\"   - Different metrics for different granularities\\\")\\n\",\n",
        "    \"print(\\\"   - Hierarchical evaluation for multi-level predictions\\\")\\n\",\n",
        "    \"print(\\\"   - Consider business impact over pure accuracy\\\")\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \\\"metadata\\\": {\n",
        "  \\\"kernelspec\\\": {\n",
        "   \\\"display_name\\\": \\\".venv\\\",\n",
        "   \\\"language\\\": \\\"python\\\",\n",
        "   \\\"name\\\": \\\"python3\\\"\n",
        "  },\n",
        "  \\\"language_info\\\": {\n",
        "   \\\"codemirror_mode\\\": {\n",
        "    \\\"name\\\": \\\"ipython\\\",\n",
        "    \\\"version\\\": 3\n",
        "   },\n",
        "   \\\"file_extension\\\": \\\".py\\\",\n",
        "   \\\"mimetype\\\": \\\"text/x-python\\\",\n",
        "   \\\"name\\\": \\\"python\\\",\n",
        "   \\\"nbconvert_exporter\\\": \\\"python\\\",\n",
        "   \\\"pygments_lexer\\\": \\\"ipython3\\\",\n",
        "   \\\"version\\\": \\\"3.12.3\\\"\n",
        "  }\n",
        " },\n",
        " \\\"nbformat\\\": 4,\n",
        " \\\"nbformat_minor\\\": 2\n",
        "}\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \".venv\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.12.3\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 2\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
