{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eab5ePbYx8g","executionInfo":{"status":"ok","timestamp":1748801917381,"user_tz":-120,"elapsed":1352,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"49b01d8c-3fd3-42f7-a2dc-408d674fd632"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-05-08T15:31:25.592287Z","iopub.status.busy":"2025-05-08T15:31:25.592058Z","iopub.status.idle":"2025-05-08T15:31:29.564860Z","shell.execute_reply":"2025-05-08T15:31:29.564195Z","shell.execute_reply.started":"2025-05-08T15:31:25.592263Z"},"trusted":true,"id":"uWOYq4SvYsjI","executionInfo":{"status":"ok","timestamp":1748801919053,"user_tz":-120,"elapsed":1670,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["import random\n","import torch\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","\n","import pickle\n","import torch\n","import tqdm\n","from torch import optim\n","from torch.utils.data import DataLoader, random_split"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.567040Z","iopub.status.busy":"2025-05-08T15:31:29.566717Z","iopub.status.idle":"2025-05-08T15:31:29.626078Z","shell.execute_reply":"2025-05-08T15:31:29.624984Z","shell.execute_reply.started":"2025-05-08T15:31:29.567021Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"VI0xrN-zYsjL","executionInfo":{"status":"ok","timestamp":1748802045223,"user_tz":-120,"elapsed":25,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"b5418fa7-f75b-44a5-864a-7e7cd104c49f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Working on DEVICE=device(type='cuda')\n"]}],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Working on {DEVICE=}')\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.627965Z","iopub.status.busy":"2025-05-08T15:31:29.627711Z","iopub.status.idle":"2025-05-08T15:31:29.649514Z","shell.execute_reply":"2025-05-08T15:31:29.648723Z","shell.execute_reply.started":"2025-05-08T15:31:29.627946Z"},"trusted":true,"id":"f05PzcL2YsjM","executionInfo":{"status":"ok","timestamp":1748802068256,"user_tz":-120,"elapsed":36,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from dataclasses import dataclass\n","import numpy as np\n","\n","\n","# GPT said this is a professional way to do it\n","@dataclass\n","class BERT4RecConfig:\n","    # EmbeddingLayer\n","    vocab_size: int\n","    embedding_dim: int\n","    max_seq_len: int\n","    embedding_dropout: float\n","\n","    # Encoder\n","    num_layers: int\n","    num_heads: int\n","    hidden_dim: int\n","    encoder_dropout: float\n","\n","    # ProjectionHead\n","    projection_dim: int\n","\n","\n","class EmbeddingLayer(nn.Module):\n","    \"\"\"Item + positional embeddings with layer normalization and dropout\"\"\"\n","\n","    def __init__(self, vocab_size: int, embedding_dim: int, max_seq_len: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.item_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.position_embeddings = nn.Embedding(max_seq_len, embedding_dim)\n","        self.layer_norm = nn.LayerNorm(embedding_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.vocab_size = vocab_size\n","\n","    def forward(self, x):\n","        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand_as(x)\n","        # print(x.max(), self.vocab_size, x.min())\n","        embeddings = self.item_embeddings(x) + self.position_embeddings(positions)\n","        embeddings = self.layer_norm(embeddings)\n","        return self.dropout(embeddings)\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"Transformer encoder. Wrapper for `torch.TransformerEncoderLayer`\"\"\"\n","\n","    def __init__(self, embedding_dim: int, num_layers: int, num_heads: int, hidden_dim: int,\n","                 dropout: float = 0.1) -> None:\n","        \"\"\"\n","\n","        Args:\n","            embedding_dim:\n","            num_layers:\n","            num_heads:\n","            hidden_dim:\n","            dropout:\n","\n","        Returns:\n","            None:\n","        \"\"\"\n","        super().__init__()\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embedding_dim,\n","            nhead=num_heads,\n","            dim_feedforward=hidden_dim,\n","            dropout=dropout,\n","            activation=\"gelu\",\n","            batch_first=True,\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","    def forward(self, x, src_key_padding_mask=None):\n","        # x: [batch_size, seq_len, embedding_dim]\n","        return self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n","\n","\n","class ProjectionHead(nn.Module):\n","    \"\"\"Projection head\"\"\"\n","\n","    def __init__(self, embedding_dim: int, projection_dim: int, vocab_size: int):\n","        super().__init__()\n","        # transform + layer normalization\n","        self.projection = nn.Linear(embedding_dim, projection_dim)\n","        self.activation = nn.GELU()\n","        self.layer_norm = nn.LayerNorm(projection_dim)\n","\n","        # map to final logits\n","        self.decoder = nn.Linear(projection_dim, vocab_size, bias=False)\n","        self.bias = nn.Parameter(torch.zeros(vocab_size))\n","\n","    def forward(self, x):\n","        # x: [batch_size, seq_len, embedding_dim]\n","        proj = self.projection(x)\n","        proj = self.activation(proj)\n","        proj = self.layer_norm(proj)\n","        return self.decoder(proj) + self.bias\n","\n","\n","class BERT4Rec(nn.Module):\n","    \"\"\"BERT4Rec model from `https://arxiv.org/pdf/1904.06690` paper\"\"\"\n","\n","    def __init__(self, config: BERT4RecConfig):\n","        super().__init__()\n","        self.embedding = EmbeddingLayer(\n","            vocab_size=config.vocab_size,\n","            embedding_dim=config.embedding_dim,\n","            max_seq_len=config.max_seq_len,\n","            dropout=config.embedding_dropout,\n","        )\n","        self.encoder = Encoder(\n","            embedding_dim=config.embedding_dim,\n","            num_layers=config.num_layers,\n","            num_heads=config.num_heads,\n","            hidden_dim=config.hidden_dim,\n","            dropout=config.encoder_dropout,\n","        )\n","        self.projection = ProjectionHead(\n","            embedding_dim=config.embedding_dim,\n","            projection_dim=config.projection_dim,\n","            vocab_size=config.vocab_size,\n","        )\n","\n","        # weights sharing\n","        self.projection.decoder.weight = self.embedding.item_embeddings.weight\n","\n","    def forward(self, x, mask=None):\n","        x = torch.clamp(x, 0, self.embedding.vocab_size - 1)\n","        pad_mask = x.eq(0)\n","        x = self.embedding(x)\n","        x = self.encoder(x, src_key_padding_mask=pad_mask)\n","        x = self.projection(x)\n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.650594Z","iopub.status.busy":"2025-05-08T15:31:29.650327Z","iopub.status.idle":"2025-05-08T15:31:29.671986Z","shell.execute_reply":"2025-05-08T15:31:29.671217Z","shell.execute_reply.started":"2025-05-08T15:31:29.650572Z"},"trusted":true,"id":"GEgTk-FLYsjM","executionInfo":{"status":"ok","timestamp":1748802068639,"user_tz":-120,"elapsed":51,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["def masked_cross_entropy_loss(logits, labels, mask, eps=1e-8):\n","    logits = logits.view(-1, logits.size(-1))\n","    labels = labels.view(-1)\n","    mask = mask.view(-1).float()\n","\n","    loss = F.cross_entropy(logits, labels, reduction='none')\n","    masked_loss = loss * mask\n","    denom = mask.sum()\n","    return masked_loss.sum() / (denom + eps)\n","\n","\n","\n","def train_step(model, optimizer, batch, device=DEVICE):\n","    model.train()\n","    input_ids, labels, masked_pos = [x.to(device) for x in batch]  # all [B, L]\n","\n","    optimizer.zero_grad()\n","    logits = model(input_ids)  # [B, L, V]\n","    loss = masked_cross_entropy_loss(logits, labels, masked_pos)\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","\n","    return loss.item()\n","\n","\n","def validate_step(model, batch, top_k=10, device=DEVICE):\n","    model.eval()\n","    input_ids, labels, masked_pos = [x.to(device) for x in batch]\n","\n","    # Comprehensive validation\n","    vocab_size = model.embedding.item_embeddings.num_embeddings\n","\n","    if input_ids.max() >= vocab_size:\n","        print(f\"ERROR: Found index {input_ids.max().item()} >= vocab_size {vocab_size}\")\n","        print(f\"Problematic indices: {input_ids[input_ids >= vocab_size]}\")\n","        # Emergency fix: clamp to valid range\n","        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n","\n","    if input_ids.min() < 0:\n","        print(f\"ERROR: Found negative index {input_ids.min().item()}\")\n","        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n","\n","\n","    try:\n","        with torch.no_grad():\n","            logits = model(input_ids)\n","    except IndexError as e:\n","        print(f\"IndexError caught: {e}\")\n","        print(f\"Input stats: min={input_ids.min()}, max={input_ids.max()}, shape={input_ids.shape}\")\n","        return 0.0, 0.0, 0.0\n","\n","    # Rest of your validation logic...\n","    masked_logits = logits[masked_pos.bool()]\n","    masked_labels = labels[masked_pos.bool()]\n","\n","    if len(masked_logits) == 0:\n","        return 0.0, 0.0, 0.0\n","\n","    loss = F.cross_entropy(masked_logits, masked_labels, reduction='mean').item()\n","\n","    _, topk = masked_logits.topk(top_k, dim=-1)\n","    hits = (topk == masked_labels.unsqueeze(1)).float()\n","    hr = hits.any(dim=1).float().mean().item()\n","    ndcg = (hits / torch.log2(torch.arange(2, top_k + 2, device=hits.device).float())).sum(dim=1).mean().item()\n","\n","    return loss, hr, ndcg\n","\n","\n","\n","def mask_sequence(seq, mask_token_id, vocab_size, mask_prob=0.15, pad_token_id=0):\n","    \"\"\"\n","    Function takes a user sequence and randomly selects items to mask.\n","    Returns:\n","        - input_ids: modified sequence with [MASK] and others.\n","        - labels: target items only in masked positions (0 elsewhere).\n","        - masked_pos: binary mask of masked locations.\n","    \"\"\"\n","    input_ids = seq.copy()\n","    labels = [0] * len(seq)\n","    masked_pos = [0] * len(seq)\n","    # mask_prob = 0.5\n","\n","    for i in range(len(seq)):\n","        if seq[i] == pad_token_id:\n","            continue\n","        if random.random() < mask_prob:\n","            masked_pos[i] = 1\n","            labels[i] = seq[i]\n","            rand = random.random()\n","            if rand < 0.8:\n","                input_ids[i] = mask_token_id\n","            elif rand < 0.9:\n","                input_ids[i] = random.randint(1, vocab_size - 1)  # avoid pad token\n","            else:\n","                pass  # leave unchanged\n","    return input_ids, labels, masked_pos\n","\n","\n","def collate_fn(batch, mask_token_id, vocab_size, pad_token_id=0, max_len=None):\n","    \"\"\"\n","    Pads sequences to same length, converts everything to tensors.\n","    batch: list of sequences (list of item IDs)\n","    \"\"\"\n","    input_ids, labels, masked_pos = [], [], []\n","\n","    for seq in batch:\n","        if max_len is not None:\n","            seq = seq[-max_len:]  # truncate if needed\n","\n","        inp, lab, msk = mask_sequence(seq, mask_token_id, vocab_size, pad_token_id=pad_token_id)\n","        input_ids.append(torch.tensor(inp))\n","        labels.append(torch.tensor(lab))\n","        masked_pos.append(torch.tensor(msk))\n","\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n","    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n","    masked_pos = pad_sequence(masked_pos, batch_first=True, padding_value=0)\n","\n","    return input_ids, labels, masked_pos"]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"/content/drive/MyDrive/recsys/bert/full_data/searchquery_bert_train.pkl\", \"rb\") as fh:\n","    train_sequences = pickle.load(fh)"],"metadata":{"id":"Ul9v2KdY7ACw","executionInfo":{"status":"ok","timestamp":1748802074320,"user_tz":-120,"elapsed":5182,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"VnVBOfxs7LvW"}},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.673079Z","iopub.status.busy":"2025-05-08T15:31:29.672827Z","iopub.status.idle":"2025-05-08T15:31:29.689995Z","shell.execute_reply":"2025-05-08T15:31:29.689252Z","shell.execute_reply.started":"2025-05-08T15:31:29.673058Z"},"trusted":true,"id":"gyfjRMEbYsjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748802078544,"user_tz":-120,"elapsed":1334,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"5a3de8ae-82f4-4afd-b96d-cb20ee7a2165"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique items count: 2389438\n","Max item ID in sequences: 2389436\n","Min item ID in sequences: 0\n","VOCAB_SIZE: 2389440\n","MASK_ID: 2389439\n"]}],"source":["max_item_id = max((el for el in train_sequences for el in el)) + 2  # Since you start enumeration from 1\n","VOCAB_SIZE = max_item_id + 2   # +1 for PAD (0), +1 for MASK\n","MASK_ID = VOCAB_SIZE - 1\n","PAD_ID = 0\n","MAX_SEQUENCE_LEN = 128\n","\n","NUM_EPOCHS = 5\n","\n","print(f\"Unique items count: {max_item_id}\")\n","print(f\"Max item ID in sequences: {max(el for seq in train_sequences for el in seq)}\")\n","print(f\"Min item ID in sequences: {min(el for seq in train_sequences for el in seq)}\")\n","print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n","print(f\"MASK_ID: {MASK_ID}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.887068Z","iopub.status.busy":"2025-05-08T15:31:29.886851Z","iopub.status.idle":"2025-05-08T15:31:29.917228Z","shell.execute_reply":"2025-05-08T15:31:29.916395Z","shell.execute_reply.started":"2025-05-08T15:31:29.887051Z"},"trusted":true,"id":"yoH5rQe_YsjN","executionInfo":{"status":"ok","timestamp":1748778398223,"user_tz":-120,"elapsed":75,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["train_size = int(0.9995 * len(train_sequences))\n","val_size = len(train_sequences) - train_size\n","\n","train_dataset, val_dataset = random_split(train_sequences, [train_size, val_size])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.918398Z","iopub.status.busy":"2025-05-08T15:31:29.918119Z","iopub.status.idle":"2025-05-08T15:31:29.924422Z","shell.execute_reply":"2025-05-08T15:31:29.923714Z","shell.execute_reply.started":"2025-05-08T15:31:29.918373Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"RvqIxaVyYsjN","executionInfo":{"status":"ok","timestamp":1748778398233,"user_tz":-120,"elapsed":7,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"6ce8f3de-c420-4560-b5cf-6b8bc00d52e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4775"]},"metadata":{},"execution_count":9}],"source":["len(train_dataset) // 256"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.925608Z","iopub.status.busy":"2025-05-08T15:31:29.925253Z","iopub.status.idle":"2025-05-08T15:31:29.940596Z","shell.execute_reply":"2025-05-08T15:31:29.939682Z","shell.execute_reply.started":"2025-05-08T15:31:29.925584Z"},"trusted":true,"id":"rMlEZ3KsYsjO","executionInfo":{"status":"ok","timestamp":1748778398243,"user_tz":-120,"elapsed":8,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["train_loader = DataLoader(\n","    dataset=train_dataset,\n","    batch_size=4,\n","    shuffle=True,\n","    collate_fn=lambda batch: collate_fn(batch, mask_token_id=MASK_ID, vocab_size=VOCAB_SIZE, max_len=MAX_SEQUENCE_LEN),\n",")\n","\n","val_loader = DataLoader(\n","    dataset=val_dataset,\n","    batch_size=8,\n","    shuffle=False,\n","    collate_fn=lambda batch: collate_fn(batch, mask_token_id=MASK_ID, vocab_size=VOCAB_SIZE, max_len=MAX_SEQUENCE_LEN),\n",")"]},{"cell_type":"code","source":["config = BERT4RecConfig(\n","    vocab_size=VOCAB_SIZE,\n","    embedding_dim=256,\n","    max_seq_len=MAX_SEQUENCE_LEN,\n","    embedding_dropout=0.2,\n","    num_layers=6,\n","    num_heads=8,\n","    hidden_dim=256,\n","    encoder_dropout=0.2,\n","    projection_dim=256,\n",")"],"metadata":{"id":"7t_lyVYuuVca","executionInfo":{"status":"ok","timestamp":1748802082480,"user_tz":-120,"elapsed":2,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:30.319116Z","iopub.status.busy":"2025-05-08T15:31:30.318840Z","iopub.status.idle":"2025-05-08T15:31:34.590098Z","shell.execute_reply":"2025-05-08T15:31:34.589335Z","shell.execute_reply.started":"2025-05-08T15:31:30.319090Z"},"trusted":true,"id":"ttXFzdTmYsjO","executionInfo":{"status":"ok","timestamp":1748778410075,"user_tz":-120,"elapsed":11829,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["config = BERT4RecConfig(\n","    vocab_size=VOCAB_SIZE,\n","    embedding_dim=256,\n","    max_seq_len=MAX_SEQUENCE_LEN,\n","    embedding_dropout=0.2,\n","    num_layers=6,\n","    num_heads=8,\n","    hidden_dim=256,\n","    encoder_dropout=0.2,\n","    projection_dim=256,\n",")\n","\n","model = BERT4Rec(config)\n","model = model.to(DEVICE)\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)"]},{"cell_type":"code","source":["\n","def evaluate(model, val_loader):\n","    model.eval()\n","    total_loss = 0.0\n","    total_hr10 = 0.0\n","    total_ndcg10 = 0.0\n","    total_batches = 0\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            val_loss, hr10, ndcg10 = validate_step(model, batch)\n","            total_loss += val_loss\n","            total_hr10 += hr10\n","            total_ndcg10 += ndcg10\n","            total_batches += 1\n","\n","    return (\n","        total_loss / total_batches,\n","        total_hr10 / total_batches,\n","        total_ndcg10 / total_batches\n","    )"],"metadata":{"id":"iQjkVmQ8wnFA","executionInfo":{"status":"ok","timestamp":1748778410079,"user_tz":-120,"elapsed":2,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:34.591237Z","iopub.status.busy":"2025-05-08T15:31:34.590870Z","iopub.status.idle":"2025-05-08T15:31:35.085293Z","shell.execute_reply":"2025-05-08T15:31:35.084189Z","shell.execute_reply.started":"2025-05-08T15:31:34.591211Z"},"trusted":true,"id":"KXEtWOrdYsjO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2a0ee80-085d-47ea-c585-3738cecf6ef5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training:   0%|          | 1002/305641 [01:28<6:53:19, 12.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 1000 | Avg Loss (last 1000): 50.9632\n"]},{"output_type":"stream","name":"stderr","text":["Training:   1%|          | 2002/305641 [02:54<7:07:04, 11.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 2000 | Avg Loss (last 1000): 37.0350\n"]},{"output_type":"stream","name":"stderr","text":["Training:   1%|          | 3003/305641 [04:21<6:58:17, 12.06it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 3000 | Avg Loss (last 1000): 27.7501\n"]},{"output_type":"stream","name":"stderr","text":["Training:   1%|▏         | 4002/305641 [05:49<6:56:15, 12.08it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 4000 | Avg Loss (last 1000): 21.6821\n"]},{"output_type":"stream","name":"stderr","text":["Training:   2%|▏         | 4999/305641 [07:15<6:38:27, 12.58it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 5000 | Avg Loss (last 1000): 18.7140\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 19.2721 | HR@10: 0.1258 | NDCG@10: 0.1203\n"]},{"output_type":"stream","name":"stderr","text":["Training:   2%|▏         | 6003/305641 [09:14<6:49:16, 12.20it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 6000 | Avg Loss (last 1000): 16.7704\n"]},{"output_type":"stream","name":"stderr","text":["Training:   2%|▏         | 7003/305641 [10:40<7:09:29, 11.59it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 7000 | Avg Loss (last 1000): 14.9184\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 8002/305641 [12:06<6:29:08, 12.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 8000 | Avg Loss (last 1000): 13.5311\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 9003/305641 [13:33<7:14:53, 11.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 9000 | Avg Loss (last 1000): 12.6130\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 10000/305641 [15:01<6:50:58, 11.99it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 10000 | Avg Loss (last 1000): 11.6926\n","\n","Epoch 1 | Val Loss: 11.8996 | HR@10: 0.1538 | NDCG@10: 0.1364\n"]},{"output_type":"stream","name":"stderr","text":["Training:   4%|▎         | 11001/305641 [16:54<6:57:25, 11.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 11000 | Avg Loss (last 1000): 11.1671\n"]},{"output_type":"stream","name":"stderr","text":["Training:   4%|▍         | 12003/305641 [18:20<6:42:51, 12.15it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 12000 | Avg Loss (last 1000): 10.9278\n"]},{"output_type":"stream","name":"stderr","text":["Training:   4%|▍         | 13003/305641 [19:47<7:06:52, 11.43it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 13000 | Avg Loss (last 1000): 10.7546\n"]},{"output_type":"stream","name":"stderr","text":["Training:   5%|▍         | 14002/305641 [21:15<7:09:10, 11.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 14000 | Avg Loss (last 1000): 10.1749\n"]},{"output_type":"stream","name":"stderr","text":["Training:   5%|▍         | 15000/305641 [22:43<7:55:52, 10.18it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 15000 | Avg Loss (last 1000): 10.4411\n","\n","Epoch 1 | Val Loss: 10.6204 | HR@10: 0.2711 | NDCG@10: 0.2378\n"]},{"output_type":"stream","name":"stderr","text":["Training:   5%|▌         | 16002/305641 [24:34<7:03:47, 11.39it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 16000 | Avg Loss (last 1000): 10.2255\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▌         | 17003/305641 [26:02<6:59:42, 11.46it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 17000 | Avg Loss (last 1000): 9.8002\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▌         | 18002/305641 [27:31<6:13:41, 12.83it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 18000 | Avg Loss (last 1000): 9.9934\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▌         | 19002/305641 [28:58<6:27:25, 12.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 19000 | Avg Loss (last 1000): 9.8312\n"]},{"output_type":"stream","name":"stderr","text":["Training:   7%|▋         | 20000/305641 [30:23<8:07:40,  9.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 20000 | Avg Loss (last 1000): 9.4062\n","\n","Epoch 1 | Val Loss: 10.3103 | HR@10: 0.2885 | NDCG@10: 0.2551\n"]},{"output_type":"stream","name":"stderr","text":["Training:   7%|▋         | 21001/305641 [32:19<6:17:47, 12.56it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 21000 | Avg Loss (last 1000): 9.3158\n"]},{"output_type":"stream","name":"stderr","text":["Training:   7%|▋         | 22002/305641 [33:46<6:10:58, 12.74it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 22000 | Avg Loss (last 1000): 9.6626\n"]},{"output_type":"stream","name":"stderr","text":["Training:   8%|▊         | 23003/305641 [35:16<6:28:50, 12.11it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 23000 | Avg Loss (last 1000): 9.9174\n"]},{"output_type":"stream","name":"stderr","text":["Training:   8%|▊         | 24002/305641 [36:45<7:29:22, 10.45it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 24000 | Avg Loss (last 1000): 9.2364\n"]},{"output_type":"stream","name":"stderr","text":["Training:   8%|▊         | 25000/305641 [38:13<7:32:42, 10.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 25000 | Avg Loss (last 1000): 9.2566\n","\n","Epoch 1 | Val Loss: 9.1797 | HR@10: 0.3891 | NDCG@10: 0.3528\n"]},{"output_type":"stream","name":"stderr","text":["Training:   9%|▊         | 26002/305641 [40:04<6:48:07, 11.42it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 26000 | Avg Loss (last 1000): 9.0909\n"]},{"output_type":"stream","name":"stderr","text":["Training:   9%|▉         | 27003/305641 [41:31<6:37:59, 11.67it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 27000 | Avg Loss (last 1000): 9.3135\n"]},{"output_type":"stream","name":"stderr","text":["Training:   9%|▉         | 28002/305641 [42:59<6:31:35, 11.82it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 28000 | Avg Loss (last 1000): 9.0889\n"]},{"output_type":"stream","name":"stderr","text":["Training:   9%|▉         | 29003/305641 [44:27<6:04:39, 12.64it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 29000 | Avg Loss (last 1000): 9.0869\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|▉         | 29999/305641 [45:54<6:49:41, 11.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 30000 | Avg Loss (last 1000): 8.8295\n","\n","Epoch 1 | Val Loss: 8.7314 | HR@10: 0.3916 | NDCG@10: 0.3579\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 31002/305641 [47:45<6:13:56, 12.24it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 31000 | Avg Loss (last 1000): 9.2420\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 32002/305641 [49:13<6:52:51, 11.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 32000 | Avg Loss (last 1000): 8.9124\n"]},{"output_type":"stream","name":"stderr","text":["Training:  11%|█         | 33001/305641 [50:40<6:52:04, 11.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 33000 | Avg Loss (last 1000): 9.0647\n"]},{"output_type":"stream","name":"stderr","text":["Training:  11%|█         | 34003/305641 [52:08<7:05:57, 10.63it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 34000 | Avg Loss (last 1000): 9.0069\n"]},{"output_type":"stream","name":"stderr","text":["Training:  11%|█▏        | 34999/305641 [53:35<6:29:19, 11.59it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 35000 | Avg Loss (last 1000): 8.9798\n"]},{"output_type":"stream","name":"stderr","text":["Training:  11%|█▏        | 35003/305641 [53:38<26:13:23,  2.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 9.0420 | HR@10: 0.3993 | NDCG@10: 0.3638\n"]},{"output_type":"stream","name":"stderr","text":["Training:  12%|█▏        | 36003/305641 [55:06<6:59:36, 10.71it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 36000 | Avg Loss (last 1000): 9.0305\n"]},{"output_type":"stream","name":"stderr","text":["Training:  12%|█▏        | 37003/305641 [56:32<6:08:03, 12.16it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 37000 | Avg Loss (last 1000): 8.8317\n"]},{"output_type":"stream","name":"stderr","text":["Training:  12%|█▏        | 38002/305641 [57:59<6:55:36, 10.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 38000 | Avg Loss (last 1000): 8.8796\n"]},{"output_type":"stream","name":"stderr","text":["Training:  13%|█▎        | 39001/305641 [59:27<5:58:52, 12.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 39000 | Avg Loss (last 1000): 8.8881\n"]},{"output_type":"stream","name":"stderr","text":["Training:  13%|█▎        | 40000/305641 [1:00:54<6:59:43, 10.55it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 40000 | Avg Loss (last 1000): 8.6649\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  13%|█▎        | 40002/305641 [1:00:57<35:14:44,  2.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 9.1256 | HR@10: 0.3961 | NDCG@10: 0.3631\n"]},{"output_type":"stream","name":"stderr","text":["Training:  13%|█▎        | 41002/305641 [1:02:25<6:09:33, 11.93it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 41000 | Avg Loss (last 1000): 8.4070\n"]},{"output_type":"stream","name":"stderr","text":["Training:  14%|█▎        | 42002/305641 [1:03:53<5:54:50, 12.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 42000 | Avg Loss (last 1000): 8.8389\n"]},{"output_type":"stream","name":"stderr","text":["Training:  14%|█▍        | 43002/305641 [1:05:20<6:08:06, 11.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 43000 | Avg Loss (last 1000): 8.8803\n"]},{"output_type":"stream","name":"stderr","text":["Training:  14%|█▍        | 44002/305641 [1:06:46<6:07:04, 11.88it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 44000 | Avg Loss (last 1000): 8.5441\n"]},{"output_type":"stream","name":"stderr","text":["Training:  15%|█▍        | 44999/305641 [1:08:12<5:34:24, 12.99it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 45000 | Avg Loss (last 1000): 8.5257\n","\n","Epoch 1 | Val Loss: 8.5392 | HR@10: 0.4477 | NDCG@10: 0.4163\n"]},{"output_type":"stream","name":"stderr","text":["Training:  15%|█▌        | 46001/305641 [1:10:04<5:50:34, 12.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 46000 | Avg Loss (last 1000): 8.7305\n"]},{"output_type":"stream","name":"stderr","text":["Training:  15%|█▌        | 47002/305641 [1:11:31<6:18:31, 11.39it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 47000 | Avg Loss (last 1000): 8.5257\n"]},{"output_type":"stream","name":"stderr","text":["Training:  16%|█▌        | 48002/305641 [1:13:00<6:01:38, 11.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 48000 | Avg Loss (last 1000): 8.5541\n"]},{"output_type":"stream","name":"stderr","text":["Training:  16%|█▌        | 49002/305641 [1:14:27<5:51:53, 12.16it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 49000 | Avg Loss (last 1000): 8.6111\n"]},{"output_type":"stream","name":"stderr","text":["Training:  16%|█▋        | 49999/305641 [1:15:55<5:43:10, 12.42it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 50000 | Avg Loss (last 1000): 8.4492\n","\n","Epoch 1 | Val Loss: 8.5121 | HR@10: 0.4310 | NDCG@10: 0.3951\n"]},{"output_type":"stream","name":"stderr","text":["Training:  17%|█▋        | 51002/305641 [1:17:50<6:30:18, 10.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 51000 | Avg Loss (last 1000): 8.4545\n"]},{"output_type":"stream","name":"stderr","text":["Training:  17%|█▋        | 52000/305641 [1:19:17<6:30:43, 10.82it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 52000 | Avg Loss (last 1000): 8.5366\n"]},{"output_type":"stream","name":"stderr","text":["Training:  17%|█▋        | 53003/305641 [1:20:43<6:06:57, 11.47it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 53000 | Avg Loss (last 1000): 8.0628\n"]},{"output_type":"stream","name":"stderr","text":["Training:  18%|█▊        | 54002/305641 [1:22:10<5:39:18, 12.36it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 54000 | Avg Loss (last 1000): 8.3118\n"]},{"output_type":"stream","name":"stderr","text":["Training:  18%|█▊        | 54999/305641 [1:23:38<6:16:13, 11.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 55000 | Avg Loss (last 1000): 8.3427\n","\n","Epoch 1 | Val Loss: 8.4584 | HR@10: 0.4573 | NDCG@10: 0.4268\n"]},{"output_type":"stream","name":"stderr","text":["Training:  18%|█▊        | 56002/305641 [1:25:35<5:39:50, 12.24it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 56000 | Avg Loss (last 1000): 8.1398\n"]},{"output_type":"stream","name":"stderr","text":["Training:  19%|█▊        | 57002/305641 [1:27:03<5:50:09, 11.83it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 57000 | Avg Loss (last 1000): 8.3991\n"]},{"output_type":"stream","name":"stderr","text":["Training:  19%|█▉        | 58003/305641 [1:28:29<5:28:11, 12.58it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 58000 | Avg Loss (last 1000): 8.3256\n"]},{"output_type":"stream","name":"stderr","text":["Training:  19%|█▉        | 59001/305641 [1:29:58<5:31:29, 12.40it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 59000 | Avg Loss (last 1000): 8.4281\n"]},{"output_type":"stream","name":"stderr","text":["Training:  20%|█▉        | 60000/305641 [1:31:26<6:15:10, 10.91it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 60000 | Avg Loss (last 1000): 8.5415\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  20%|█▉        | 60002/305641 [1:31:29<32:28:15,  2.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.7700 | HR@10: 0.4497 | NDCG@10: 0.4124\n"]},{"output_type":"stream","name":"stderr","text":["Training:  20%|█▉        | 61003/305641 [1:32:57<5:40:46, 11.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 61000 | Avg Loss (last 1000): 8.2790\n"]},{"output_type":"stream","name":"stderr","text":["Training:  20%|██        | 62001/305641 [1:34:24<5:56:42, 11.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 62000 | Avg Loss (last 1000): 8.6308\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 63003/305641 [1:35:53<5:24:14, 12.47it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 63000 | Avg Loss (last 1000): 8.4153\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 64003/305641 [1:37:19<5:56:17, 11.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 64000 | Avg Loss (last 1000): 8.2782\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██▏       | 65000/305641 [1:38:47<5:12:58, 12.81it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 65000 | Avg Loss (last 1000): 8.3542\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  21%|██▏       | 65002/305641 [1:38:50<31:36:45,  2.11it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 9.2241 | HR@10: 0.4144 | NDCG@10: 0.3787\n"]},{"output_type":"stream","name":"stderr","text":["Training:  22%|██▏       | 66003/305641 [1:40:17<5:22:51, 12.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 66000 | Avg Loss (last 1000): 8.4339\n"]},{"output_type":"stream","name":"stderr","text":["Training:  22%|██▏       | 67003/305641 [1:41:45<5:17:29, 12.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 67000 | Avg Loss (last 1000): 8.4413\n"]},{"output_type":"stream","name":"stderr","text":["Training:  22%|██▏       | 68002/305641 [1:43:11<5:57:40, 11.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 68000 | Avg Loss (last 1000): 8.2006\n"]},{"output_type":"stream","name":"stderr","text":["Training:  23%|██▎       | 69001/305641 [1:44:38<5:55:41, 11.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 69000 | Avg Loss (last 1000): 8.3797\n"]},{"output_type":"stream","name":"stderr","text":["Training:  23%|██▎       | 70000/305641 [1:46:06<6:44:32,  9.71it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 70000 | Avg Loss (last 1000): 8.3406\n","\n","Epoch 1 | Val Loss: 8.4131 | HR@10: 0.4525 | NDCG@10: 0.4269\n"]},{"output_type":"stream","name":"stderr","text":["Training:  23%|██▎       | 71003/305641 [1:47:57<5:21:32, 12.16it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 71000 | Avg Loss (last 1000): 8.5703\n"]},{"output_type":"stream","name":"stderr","text":["Training:  24%|██▎       | 72003/305641 [1:49:24<5:20:46, 12.14it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 72000 | Avg Loss (last 1000): 8.2005\n"]},{"output_type":"stream","name":"stderr","text":["Training:  24%|██▍       | 73002/305641 [1:50:51<5:45:44, 11.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 73000 | Avg Loss (last 1000): 7.9376\n"]},{"output_type":"stream","name":"stderr","text":["Training:  24%|██▍       | 74002/305641 [1:52:18<5:14:35, 12.27it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 74000 | Avg Loss (last 1000): 7.9151\n"]},{"output_type":"stream","name":"stderr","text":["Training:  25%|██▍       | 74999/305641 [1:53:46<5:27:48, 11.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 75000 | Avg Loss (last 1000): 8.3497\n"]},{"output_type":"stream","name":"stderr","text":["Training:  25%|██▍       | 75003/305641 [1:53:49<23:06:00,  2.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.4349 | HR@10: 0.4579 | NDCG@10: 0.4215\n"]},{"output_type":"stream","name":"stderr","text":["Training:  25%|██▍       | 76002/305641 [1:55:16<5:07:54, 12.43it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 76000 | Avg Loss (last 1000): 8.2596\n"]},{"output_type":"stream","name":"stderr","text":["Training:  25%|██▌       | 77003/305641 [1:56:46<5:15:35, 12.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 77000 | Avg Loss (last 1000): 8.4379\n"]},{"output_type":"stream","name":"stderr","text":["Training:  26%|██▌       | 78003/305641 [1:58:11<5:17:16, 11.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 78000 | Avg Loss (last 1000): 8.2324\n"]},{"output_type":"stream","name":"stderr","text":["Training:  26%|██▌       | 79002/305641 [1:59:38<4:52:06, 12.93it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 79000 | Avg Loss (last 1000): 8.0525\n"]},{"output_type":"stream","name":"stderr","text":["Training:  26%|██▌       | 80000/305641 [2:01:05<4:59:24, 12.56it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 80000 | Avg Loss (last 1000): 8.1473\n","\n","Epoch 1 | Val Loss: 8.2686 | HR@10: 0.4504 | NDCG@10: 0.4144\n"]},{"output_type":"stream","name":"stderr","text":["Training:  27%|██▋       | 81002/305641 [2:02:56<5:24:35, 11.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 81000 | Avg Loss (last 1000): 8.2028\n"]},{"output_type":"stream","name":"stderr","text":["Training:  27%|██▋       | 82002/305641 [2:04:24<5:14:14, 11.86it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 82000 | Avg Loss (last 1000): 8.1595\n"]},{"output_type":"stream","name":"stderr","text":["Training:  27%|██▋       | 83003/305641 [2:05:51<5:01:36, 12.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 83000 | Avg Loss (last 1000): 7.7671\n"]},{"output_type":"stream","name":"stderr","text":["Training:  27%|██▋       | 84002/305641 [2:07:19<5:11:09, 11.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 84000 | Avg Loss (last 1000): 8.2077\n"]},{"output_type":"stream","name":"stderr","text":["Training:  28%|██▊       | 85000/305641 [2:08:45<4:38:31, 13.20it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 85000 | Avg Loss (last 1000): 8.2249\n","\n","Epoch 1 | Val Loss: 7.9131 | HR@10: 0.4823 | NDCG@10: 0.4547\n"]},{"output_type":"stream","name":"stderr","text":["Training:  28%|██▊       | 86002/305641 [2:10:40<5:38:42, 10.81it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 86000 | Avg Loss (last 1000): 8.1654\n"]},{"output_type":"stream","name":"stderr","text":["Training:  28%|██▊       | 87002/305641 [2:12:08<5:56:26, 10.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 87000 | Avg Loss (last 1000): 8.3428\n"]},{"output_type":"stream","name":"stderr","text":["Training:  29%|██▉       | 88001/305641 [2:13:34<5:04:57, 11.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 88000 | Avg Loss (last 1000): 7.9466\n"]},{"output_type":"stream","name":"stderr","text":["Training:  29%|██▉       | 89003/305641 [2:15:02<5:34:04, 10.81it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 89000 | Avg Loss (last 1000): 7.9108\n"]},{"output_type":"stream","name":"stderr","text":["Training:  29%|██▉       | 90000/305641 [2:16:29<4:57:03, 12.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 90000 | Avg Loss (last 1000): 7.7785\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  29%|██▉       | 90002/305641 [2:16:32<28:52:41,  2.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.7611 | HR@10: 0.4569 | NDCG@10: 0.4146\n"]},{"output_type":"stream","name":"stderr","text":["Training:  30%|██▉       | 91003/305641 [2:18:00<4:38:11, 12.86it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 91000 | Avg Loss (last 1000): 7.9825\n"]},{"output_type":"stream","name":"stderr","text":["Training:  30%|███       | 92002/305641 [2:19:27<6:06:33,  9.71it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 92000 | Avg Loss (last 1000): 8.2221\n"]},{"output_type":"stream","name":"stderr","text":["Training:  30%|███       | 93002/305641 [2:20:53<5:17:45, 11.15it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 93000 | Avg Loss (last 1000): 7.8554\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 94003/305641 [2:22:21<4:46:28, 12.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 94000 | Avg Loss (last 1000): 8.2012\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 95000/305641 [2:23:50<5:50:25, 10.02it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 95000 | Avg Loss (last 1000): 8.3161\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  31%|███       | 95002/305641 [2:23:53<27:57:50,  2.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.0899 | HR@10: 0.4933 | NDCG@10: 0.4483\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███▏      | 96002/305641 [2:25:20<5:04:02, 11.49it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 96000 | Avg Loss (last 1000): 8.2442\n"]},{"output_type":"stream","name":"stderr","text":["Training:  32%|███▏      | 97003/305641 [2:26:48<5:11:59, 11.15it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 97000 | Avg Loss (last 1000): 8.2322\n"]},{"output_type":"stream","name":"stderr","text":["Training:  32%|███▏      | 98003/305641 [2:28:16<4:41:15, 12.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 98000 | Avg Loss (last 1000): 8.0418\n"]},{"output_type":"stream","name":"stderr","text":["Training:  32%|███▏      | 99003/305641 [2:29:44<4:49:42, 11.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 99000 | Avg Loss (last 1000): 7.9565\n"]},{"output_type":"stream","name":"stderr","text":["Training:  33%|███▎      | 100000/305641 [2:31:09<4:24:45, 12.94it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 100000 | Avg Loss (last 1000): 7.9805\n","\n","Epoch 1 | Val Loss: 7.7317 | HR@10: 0.5186 | NDCG@10: 0.4853\n"]},{"output_type":"stream","name":"stderr","text":["Training:  33%|███▎      | 101003/305641 [2:33:05<4:35:39, 12.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 101000 | Avg Loss (last 1000): 7.8707\n"]},{"output_type":"stream","name":"stderr","text":["Training:  33%|███▎      | 102002/305641 [2:34:33<4:58:22, 11.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 102000 | Avg Loss (last 1000): 8.2133\n"]},{"output_type":"stream","name":"stderr","text":["Training:  34%|███▎      | 103002/305641 [2:36:00<4:42:29, 11.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 103000 | Avg Loss (last 1000): 8.2009\n"]},{"output_type":"stream","name":"stderr","text":["Training:  34%|███▍      | 104002/305641 [2:37:27<4:40:56, 11.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 104000 | Avg Loss (last 1000): 8.1044\n"]},{"output_type":"stream","name":"stderr","text":["Training:  34%|███▍      | 105000/305641 [2:38:53<4:19:23, 12.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 105000 | Avg Loss (last 1000): 8.2029\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  34%|███▍      | 105002/305641 [2:38:56<25:49:41,  2.16it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.2355 | HR@10: 0.4620 | NDCG@10: 0.4357\n"]},{"output_type":"stream","name":"stderr","text":["Training:  35%|███▍      | 106003/305641 [2:40:23<5:23:38, 10.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 106000 | Avg Loss (last 1000): 7.9990\n"]},{"output_type":"stream","name":"stderr","text":["Training:  35%|███▌      | 107002/305641 [2:41:51<4:32:50, 12.13it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 107000 | Avg Loss (last 1000): 8.2226\n"]},{"output_type":"stream","name":"stderr","text":["Training:  35%|███▌      | 108003/305641 [2:43:19<4:34:58, 11.98it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 108000 | Avg Loss (last 1000): 8.0939\n"]},{"output_type":"stream","name":"stderr","text":["Training:  36%|███▌      | 109003/305641 [2:44:44<4:52:13, 11.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 109000 | Avg Loss (last 1000): 7.7293\n"]},{"output_type":"stream","name":"stderr","text":["Training:  36%|███▌      | 109999/305641 [2:46:12<4:37:54, 11.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 110000 | Avg Loss (last 1000): 7.9679\n"]},{"output_type":"stream","name":"stderr","text":["Training:  36%|███▌      | 110003/305641 [2:46:15<19:45:50,  2.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.3514 | HR@10: 0.4730 | NDCG@10: 0.4382\n"]},{"output_type":"stream","name":"stderr","text":["Training:  36%|███▋      | 111003/305641 [2:47:42<4:11:27, 12.90it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 111000 | Avg Loss (last 1000): 8.3225\n"]},{"output_type":"stream","name":"stderr","text":["Training:  37%|███▋      | 112003/305641 [2:49:08<4:18:12, 12.50it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 112000 | Avg Loss (last 1000): 8.2069\n"]},{"output_type":"stream","name":"stderr","text":["Training:  37%|███▋      | 113003/305641 [2:50:35<4:37:35, 11.57it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 113000 | Avg Loss (last 1000): 8.3517\n"]},{"output_type":"stream","name":"stderr","text":["Training:  37%|███▋      | 114002/305641 [2:52:02<5:30:06,  9.68it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 114000 | Avg Loss (last 1000): 8.3749\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 114999/305641 [2:53:30<4:59:42, 10.60it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 115000 | Avg Loss (last 1000): 7.7363\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 115003/305641 [2:53:32<19:01:11,  2.78it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.9961 | HR@10: 0.5033 | NDCG@10: 0.4678\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 116003/305641 [2:55:00<4:38:35, 11.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 116000 | Avg Loss (last 1000): 8.0932\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 117003/305641 [2:56:28<4:34:34, 11.45it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 117000 | Avg Loss (last 1000): 7.6972\n"]},{"output_type":"stream","name":"stderr","text":["Training:  39%|███▊      | 118002/305641 [2:57:55<4:11:55, 12.41it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 118000 | Avg Loss (last 1000): 7.9847\n"]},{"output_type":"stream","name":"stderr","text":["Training:  39%|███▉      | 119002/305641 [2:59:23<4:40:34, 11.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 119000 | Avg Loss (last 1000): 8.2499\n"]},{"output_type":"stream","name":"stderr","text":["Training:  39%|███▉      | 120000/305641 [3:00:49<4:42:19, 10.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 120000 | Avg Loss (last 1000): 7.7583\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  39%|███▉      | 120002/305641 [3:00:52<24:30:31,  2.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.1268 | HR@10: 0.4892 | NDCG@10: 0.4585\n"]},{"output_type":"stream","name":"stderr","text":["Training:  40%|███▉      | 121002/305641 [3:02:19<3:56:07, 13.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 121000 | Avg Loss (last 1000): 8.0293\n"]},{"output_type":"stream","name":"stderr","text":["Training:  40%|███▉      | 122003/305641 [3:03:47<4:21:50, 11.69it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 122000 | Avg Loss (last 1000): 8.0087\n"]},{"output_type":"stream","name":"stderr","text":["Training:  40%|████      | 123002/305641 [3:05:14<4:00:10, 12.67it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 123000 | Avg Loss (last 1000): 8.0107\n"]},{"output_type":"stream","name":"stderr","text":["Training:  41%|████      | 124002/305641 [3:06:43<4:34:02, 11.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 124000 | Avg Loss (last 1000): 7.8753\n"]},{"output_type":"stream","name":"stderr","text":["Training:  41%|████      | 125000/305641 [3:08:10<4:54:22, 10.23it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 125000 | Avg Loss (last 1000): 7.8448\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  41%|████      | 125002/305641 [3:08:13<24:13:26,  2.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.2692 | HR@10: 0.4869 | NDCG@10: 0.4581\n"]},{"output_type":"stream","name":"stderr","text":["Training:  41%|████      | 126003/305641 [3:09:41<4:06:02, 12.17it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 126000 | Avg Loss (last 1000): 8.1643\n"]},{"output_type":"stream","name":"stderr","text":["Training:  42%|████▏     | 127003/305641 [3:11:08<4:25:09, 11.23it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 127000 | Avg Loss (last 1000): 7.6271\n"]},{"output_type":"stream","name":"stderr","text":["Training:  42%|████▏     | 128002/305641 [3:12:37<5:26:53,  9.06it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 128000 | Avg Loss (last 1000): 7.7714\n"]},{"output_type":"stream","name":"stderr","text":["Training:  42%|████▏     | 129003/305641 [3:14:06<4:04:38, 12.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 129000 | Avg Loss (last 1000): 7.8907\n"]},{"output_type":"stream","name":"stderr","text":["Training:  43%|████▎     | 129999/305641 [3:15:32<3:46:26, 12.93it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 130000 | Avg Loss (last 1000): 7.7881\n"]},{"output_type":"stream","name":"stderr","text":["Training:  43%|████▎     | 130003/305641 [3:15:35<17:37:55,  2.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.8218 | HR@10: 0.4862 | NDCG@10: 0.4683\n"]},{"output_type":"stream","name":"stderr","text":["Training:  43%|████▎     | 131003/305641 [3:17:02<4:08:43, 11.70it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 131000 | Avg Loss (last 1000): 8.0122\n"]},{"output_type":"stream","name":"stderr","text":["Training:  43%|████▎     | 132002/305641 [3:18:30<3:49:38, 12.60it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 132000 | Avg Loss (last 1000): 7.7985\n"]},{"output_type":"stream","name":"stderr","text":["Training:  44%|████▎     | 133001/305641 [3:19:58<3:50:26, 12.49it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 133000 | Avg Loss (last 1000): 7.6823\n"]},{"output_type":"stream","name":"stderr","text":["Training:  44%|████▍     | 134003/305641 [3:21:25<3:58:36, 11.99it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 134000 | Avg Loss (last 1000): 8.0408\n"]},{"output_type":"stream","name":"stderr","text":["Training:  44%|████▍     | 135000/305641 [3:22:52<4:17:09, 11.06it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 135000 | Avg Loss (last 1000): 7.8313\n","\n","Epoch 1 | Val Loss: 7.3976 | HR@10: 0.5055 | NDCG@10: 0.4811\n"]},{"output_type":"stream","name":"stderr","text":["Training:  44%|████▍     | 136002/305641 [3:24:54<4:08:42, 11.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 136000 | Avg Loss (last 1000): 8.0281\n"]},{"output_type":"stream","name":"stderr","text":["Training:  45%|████▍     | 137002/305641 [3:26:21<4:13:30, 11.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 137000 | Avg Loss (last 1000): 7.6327\n"]},{"output_type":"stream","name":"stderr","text":["Training:  45%|████▌     | 138002/305641 [3:27:50<4:33:16, 10.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 138000 | Avg Loss (last 1000): 7.8902\n"]},{"output_type":"stream","name":"stderr","text":["Training:  45%|████▌     | 139002/305641 [3:29:18<3:46:12, 12.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 139000 | Avg Loss (last 1000): 7.8184\n"]},{"output_type":"stream","name":"stderr","text":["Training:  46%|████▌     | 139999/305641 [3:30:45<3:44:06, 12.32it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 140000 | Avg Loss (last 1000): 7.9447\n"]},{"output_type":"stream","name":"stderr","text":["Training:  46%|████▌     | 140002/305641 [3:30:48<18:32:11,  2.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.6513 | HR@10: 0.5059 | NDCG@10: 0.4778\n"]},{"output_type":"stream","name":"stderr","text":["Training:  46%|████▌     | 141002/305641 [3:32:14<3:52:48, 11.79it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 141000 | Avg Loss (last 1000): 7.9440\n"]},{"output_type":"stream","name":"stderr","text":["Training:  46%|████▋     | 142002/305641 [3:33:42<3:36:44, 12.58it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 142000 | Avg Loss (last 1000): 8.0418\n"]},{"output_type":"stream","name":"stderr","text":["Training:  47%|████▋     | 143003/305641 [3:35:11<3:39:55, 12.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 143000 | Avg Loss (last 1000): 7.8619\n"]},{"output_type":"stream","name":"stderr","text":["Training:  47%|████▋     | 144003/305641 [3:36:38<3:49:53, 11.72it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 144000 | Avg Loss (last 1000): 8.0343\n"]},{"output_type":"stream","name":"stderr","text":["Training:  47%|████▋     | 144999/305641 [3:38:04<4:04:20, 10.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 145000 | Avg Loss (last 1000): 7.6663\n"]},{"output_type":"stream","name":"stderr","text":["Training:  47%|████▋     | 145002/305641 [3:38:07<18:43:29,  2.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.7651 | HR@10: 0.4846 | NDCG@10: 0.4612\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 146002/305641 [3:39:34<3:40:22, 12.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 146000 | Avg Loss (last 1000): 8.1064\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 147003/305641 [3:41:02<4:09:13, 10.61it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 147000 | Avg Loss (last 1000): 8.1684\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 148002/305641 [3:42:31<3:54:22, 11.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 148000 | Avg Loss (last 1000): 7.8106\n"]},{"output_type":"stream","name":"stderr","text":["Training:  49%|████▉     | 149003/305641 [3:44:00<3:45:30, 11.58it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 149000 | Avg Loss (last 1000): 7.9530\n"]},{"output_type":"stream","name":"stderr","text":["Training:  49%|████▉     | 149999/305641 [3:45:27<4:19:48,  9.98it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 150000 | Avg Loss (last 1000): 7.8023\n"]},{"output_type":"stream","name":"stderr","text":["Training:  49%|████▉     | 150003/305641 [3:45:30<15:32:08,  2.78it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.2096 | HR@10: 0.5104 | NDCG@10: 0.4730\n"]},{"output_type":"stream","name":"stderr","text":["Training:  49%|████▉     | 151002/305641 [3:46:57<4:02:55, 10.61it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 151000 | Avg Loss (last 1000): 7.7822\n"]},{"output_type":"stream","name":"stderr","text":["Training:  50%|████▉     | 152002/305641 [3:48:25<4:23:57,  9.70it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 152000 | Avg Loss (last 1000): 7.7797\n"]},{"output_type":"stream","name":"stderr","text":["Training:  50%|█████     | 153003/305641 [3:49:52<3:39:41, 11.58it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 153000 | Avg Loss (last 1000): 7.8304\n"]},{"output_type":"stream","name":"stderr","text":["Training:  50%|█████     | 154002/305641 [3:51:20<3:55:44, 10.72it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 154000 | Avg Loss (last 1000): 7.9234\n"]},{"output_type":"stream","name":"stderr","text":["Training:  51%|█████     | 155000/305641 [3:52:47<3:29:10, 12.00it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 155000 | Avg Loss (last 1000): 7.8344\n","\n","Epoch 1 | Val Loss: 8.4519 | HR@10: 0.4672 | NDCG@10: 0.4270\n"]},{"output_type":"stream","name":"stderr","text":["Training:  51%|█████     | 156002/305641 [3:54:17<3:19:54, 12.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 156000 | Avg Loss (last 1000): 8.0792\n"]},{"output_type":"stream","name":"stderr","text":["Training:  51%|█████▏    | 157002/305641 [3:55:46<3:33:13, 11.62it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 157000 | Avg Loss (last 1000): 7.8070\n"]},{"output_type":"stream","name":"stderr","text":["Training:  52%|█████▏    | 158002/305641 [3:57:13<3:46:08, 10.88it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 158000 | Avg Loss (last 1000): 7.9552\n"]},{"output_type":"stream","name":"stderr","text":["Training:  52%|█████▏    | 159001/305641 [3:58:40<3:25:38, 11.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 159000 | Avg Loss (last 1000): 7.6093\n"]},{"output_type":"stream","name":"stderr","text":["Training:  52%|█████▏    | 159999/305641 [4:00:07<3:42:14, 10.92it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 160000 | Avg Loss (last 1000): 7.9934\n"]},{"output_type":"stream","name":"stderr","text":["Training:  52%|█████▏    | 160003/305641 [4:00:10<14:45:54,  2.74it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.9046 | HR@10: 0.5103 | NDCG@10: 0.4705\n"]},{"output_type":"stream","name":"stderr","text":["Training:  53%|█████▎    | 161003/305641 [4:01:38<3:30:22, 11.46it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 161000 | Avg Loss (last 1000): 7.8812\n"]},{"output_type":"stream","name":"stderr","text":["Training:  53%|█████▎    | 162003/305641 [4:03:05<3:27:40, 11.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 162000 | Avg Loss (last 1000): 7.9393\n"]},{"output_type":"stream","name":"stderr","text":["Training:  53%|█████▎    | 163003/305641 [4:04:33<3:07:11, 12.70it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 163000 | Avg Loss (last 1000): 8.1695\n"]},{"output_type":"stream","name":"stderr","text":["Training:  54%|█████▎    | 164003/305641 [4:06:00<3:17:29, 11.95it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 164000 | Avg Loss (last 1000): 7.7607\n"]},{"output_type":"stream","name":"stderr","text":["Training:  54%|█████▍    | 164999/305641 [4:07:28<3:15:18, 12.00it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 165000 | Avg Loss (last 1000): 7.7600\n"]},{"output_type":"stream","name":"stderr","text":["Training:  54%|█████▍    | 165002/305641 [4:07:30<15:53:16,  2.46it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.9360 | HR@10: 0.4846 | NDCG@10: 0.4539\n"]},{"output_type":"stream","name":"stderr","text":["Training:  54%|█████▍    | 166003/305641 [4:08:59<3:00:19, 12.91it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 166000 | Avg Loss (last 1000): 7.6288\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▍    | 167001/305641 [4:10:26<2:58:59, 12.91it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 167000 | Avg Loss (last 1000): 7.8967\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▍    | 168003/305641 [4:11:55<2:59:18, 12.79it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 168000 | Avg Loss (last 1000): 7.9241\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▌    | 169003/305641 [4:13:21<3:05:07, 12.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 169000 | Avg Loss (last 1000): 7.8842\n"]},{"output_type":"stream","name":"stderr","text":["Training:  56%|█████▌    | 170000/305641 [4:14:51<3:28:11, 10.86it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 170000 | Avg Loss (last 1000): 7.8642\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:  56%|█████▌    | 170002/305641 [4:14:53<17:56:08,  2.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 7.9138 | HR@10: 0.4995 | NDCG@10: 0.4647\n"]},{"output_type":"stream","name":"stderr","text":["Training:  56%|█████▌    | 171003/305641 [4:16:21<3:12:32, 11.65it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 171000 | Avg Loss (last 1000): 7.5970\n"]},{"output_type":"stream","name":"stderr","text":["Training:  56%|█████▋    | 172002/305641 [4:17:49<3:44:57,  9.90it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 172000 | Avg Loss (last 1000): 8.1171\n"]},{"output_type":"stream","name":"stderr","text":["Training:  57%|█████▋    | 173002/305641 [4:19:16<3:07:54, 11.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 173000 | Avg Loss (last 1000): 7.8027\n"]},{"output_type":"stream","name":"stderr","text":["Training:  57%|█████▋    | 174003/305641 [4:20:43<3:02:40, 12.01it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 174000 | Avg Loss (last 1000): 7.7399\n"]},{"output_type":"stream","name":"stderr","text":["Training:  57%|█████▋    | 174999/305641 [4:22:10<2:57:14, 12.29it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 175000 | Avg Loss (last 1000): 7.9556\n"]},{"output_type":"stream","name":"stderr","text":["Training:  57%|█████▋    | 175003/305641 [4:22:13<12:34:13,  2.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 | Val Loss: 8.2885 | HR@10: 0.4926 | NDCG@10: 0.4591\n"]},{"output_type":"stream","name":"stderr","text":["Training:  58%|█████▊    | 176003/305641 [4:23:40<3:36:25,  9.98it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 176000 | Avg Loss (last 1000): 7.6239\n"]},{"output_type":"stream","name":"stderr","text":["Training:  58%|█████▊    | 177002/305641 [4:25:08<3:29:03, 10.26it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Step 177000 | Avg Loss (last 1000): 8.0110\n"]},{"output_type":"stream","name":"stderr","text":["Training:  58%|█████▊    | 177708/305641 [4:26:10<3:14:01, 10.99it/s]"]}],"source":["train_history = []\n","valid_history = []\n","\n","NUM_EPOCHS = 3\n","for epoch in range(NUM_EPOCHS):\n","    for step, batch in enumerate(tqdm.tqdm(train_loader, desc=\"Training\")):\n","        loss = train_step(model, optimizer, batch)\n","        train_history.append(loss)\n","\n","        if step % 1000 == 0 and step != 0:\n","            print(f\"\\nStep {step} | Avg Loss (last 1000): {np.mean(train_history[-1000:]):.4f}\")\n","\n","        if step % 5000 == 0 and step != 0:\n","            val_loss, hr10, ndcg10 = evaluate(model, val_loader)\n","            valid_history.append((val_loss, hr10, ndcg10))\n","            print(f\"\\nEpoch {epoch + 1} | Val Loss: {val_loss:.4f} | HR@10: {hr10:.4f} | NDCG@10: {ndcg10:.4f}\")\n","\n","            if val_loss == min(valid_history, key=lambda x: x[0])[0]:\n","                torch.save({\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'config': config\n","                }, '/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_searchquery.pt')\n","\n","    scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2025-05-08T15:31:35.085950Z","iopub.status.idle":"2025-05-08T15:31:35.086173Z","shell.execute_reply":"2025-05-08T15:31:35.086081Z","shell.execute_reply.started":"2025-05-08T15:31:35.086071Z"},"trusted":true,"id":"yRJYor48YsjO"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def moving_average(data, window_size):\n","    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n","\n","window_size = 100\n","y = moving_average(train_history, window_size)\n","x = np.arange(len(y)) + window_size\n","\n","plt.plot(x, y, color='blue', alpha=0.5, label='train loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","plt.axhline(val_loss, color='black', linestyle='--', label='val loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1_C4sogYsjO"},"outputs":[],"source":["torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'config': config\n","}, '/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_searchquery.pt')\n"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pickle\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","# Load the saved model\n","print(\"Loading model...\")\n","checkpoint = torch.load('/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_searchquery.pt', weights_only=False)\n","\n","# Initialize model with saved config\n","model = BERT4Rec(checkpoint['config'])\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Move to device and set to eval mode\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(DEVICE)\n","model.eval()\n","\n","print(\"Model loaded successfully!\")\n","\n","# Get configuration\n","PAD = 0\n","max_len = MAX_SEQUENCE_LEN\n","\n","# Load and filter data\n","print(\"Loading training data...\")\n","with open(\"/content/drive/MyDrive/recsys/bert/full_data/searchquery_bert_train.pkl\", \"rb\") as fh:\n","    train_sequences = pickle.load(fh)\n","\n","with open(\"/content/drive/MyDrive/recsys/bert/full_data/searchquery_clients_train.pkl\", \"rb\") as fh:\n","    clients = pickle.load(fh)\n","\n","relevant_clients = np.load(\"/content/drive/MyDrive/recsys/original_data/input/relevant_clients.npy\")\n","\n","print(f\"Total sequences: {len(train_sequences)}\")\n","print(f\"Relevant clients: {len(relevant_clients)}\")\n","\n","# Convert to sets for efficient lookup\n","relevant_client_set = set(relevant_clients)\n","available_client_set = set(clients)\n","\n","# Find clients that are in relevant_clients but not in available clients\n","missing_clients = relevant_client_set - available_client_set\n","print(f\"Missing clients (in relevant but not in training): {len(missing_clients)}\")\n","\n","# Filter sequences for relevant clients that exist in training data\n","filtered_data = []\n","filtered_client_ids = []\n","\n","for i, client in enumerate(clients):\n","    if client in relevant_client_set:\n","        filtered_data.append(train_sequences[i])\n","        filtered_client_ids.append(client)\n","\n","print(f\"Filtered sequences with data: {len(filtered_data)}\")\n","\n","# Pad sequences and create attention masks\n","def pad_sequence_to_length(sequence, target_length, pad_value=PAD):\n","    if len(sequence) >= target_length:\n","        return sequence[:target_length]\n","    else:\n","        return sequence + [pad_value] * (target_length - len(sequence))\n","\n","padded_sequences = []\n","attention_masks = []\n","\n","for seq in filtered_data:\n","    padded_seq = pad_sequence_to_length(seq, max_len, PAD)\n","    mask = [1 if token != PAD else 0 for token in padded_seq]\n","    padded_sequences.append(padded_seq)\n","    attention_masks.append(mask)\n","\n","# Generate embeddings for clients with data\n","print(\"Generating embeddings for clients with data...\")\n","batch_size = 512\n","computed_embeddings = []\n","\n","if len(filtered_data) > 0:\n","    padded_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n","    mask_tensor = torch.tensor(attention_masks, dtype=torch.float)\n","    dataset = TensorDataset(padded_tensor, mask_tensor)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    def mean_pooling(embeddings, attention_mask):\n","        \"\"\"Apply mean pooling with attention mask to ignore padding tokens\"\"\"\n","        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size())\n","        sum_embeddings = torch.sum(embeddings * mask_expanded, dim=1)\n","        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (batch_sequences, batch_masks) in enumerate(dataloader):\n","            batch_sequences = batch_sequences.to(DEVICE)\n","            batch_masks = batch_masks.to(DEVICE)\n","\n","            embeddings = model.embedding(batch_sequences)\n","            embeddings = model.encoder(embeddings)\n","\n","            pooled_embeddings = mean_pooling(embeddings, batch_masks)\n","            pooled_embeddings = pooled_embeddings.cpu().numpy().astype(np.float16)\n","            computed_embeddings.append(pooled_embeddings)\n","\n","            if (batch_idx + 1) % 10 == 0:\n","                print(f\"Processed {(batch_idx + 1) * batch_size} sequences...\")\n","\n","    computed_embeddings = np.concatenate(computed_embeddings, axis=0)\n","else:\n","    computed_embeddings = np.empty((0, 64), dtype=np.float16)\n","\n","print(f\"Computed embeddings shape: {computed_embeddings.shape}\")\n","\n","# Create complete embeddings array for all relevant clients\n","print(\"Creating complete embeddings array...\")\n","\n","# Sort relevant clients for consistent ordering\n","sorted_relevant_clients = np.sort(relevant_clients)\n","\n","# Create mapping from client_id to embedding\n","client_to_embedding = {}\n","for i, client_id in enumerate(filtered_client_ids):\n","    client_to_embedding[client_id] = computed_embeddings[i]\n","\n","# Create final arrays\n","final_embeddings = []\n","final_client_ids = []\n","\n","for client_id in sorted_relevant_clients:\n","    final_client_ids.append(client_id)\n","\n","    if client_id in client_to_embedding:\n","        # Use computed embedding\n","        final_embeddings.append(client_to_embedding[client_id])\n","    else:\n","        # Use zero embedding for missing clients\n","        zero_embedding = np.zeros(256, dtype=np.float16)\n","        final_embeddings.append(zero_embedding)\n","\n","# Convert to numpy arrays\n","final_embeddings = np.array(final_embeddings, dtype=np.float16)\n","final_client_ids = np.array(final_client_ids, dtype=np.int64)\n","\n","print(f\"Final embeddings shape: {final_embeddings.shape}\")\n","print(f\"Final client IDs shape: {final_client_ids.shape}\")\n","print(f\"Clients with computed embeddings: {len(filtered_client_ids)}\")\n","print(f\"Clients with zero embeddings: {len(missing_clients)}\")\n","print(f\"Total clients: {len(final_client_ids)}\")\n","\n","# Verify all relevant clients are included\n","assert len(final_client_ids) == len(relevant_clients), \"Mismatch in client count!\"\n","assert set(final_client_ids) == set(relevant_clients), \"Client sets don't match!\"\n","\n","# Save the results\n","print(\"Saving complete embeddings and client IDs...\")\n","np.save('/content/drive/MyDrive/recsys/bert/full_data/embeddings_512dim_bigger_30seqlen_searchquery.npy', final_embeddings)\n","np.save('/content/drive/MyDrive/recsys/bert/full_data/client_ids_512dim_bigger_30seqlen_searchquery.npy', final_client_ids)\n","\n","print(\"Complete embeddings generation finished!\")\n","print(f\"Final shape: [{len(final_client_ids)}, {64}]\")\n"],"metadata":{"id":"k0-KBdjDEY7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748802239931,"user_tz":-120,"elapsed":59773,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"9ec29697-3e11-41c5-b3a2-7dc42eeac2bd"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Model loaded successfully!\n","Loading training data...\n","Total sequences: 1223173\n","Relevant clients: 1000000\n","Missing clients (in relevant but not in training): 676832\n","Filtered sequences with data: 323168\n","Generating embeddings for clients with data...\n","Processed 5120 sequences...\n","Processed 10240 sequences...\n","Processed 15360 sequences...\n","Processed 20480 sequences...\n","Processed 25600 sequences...\n","Processed 30720 sequences...\n","Processed 35840 sequences...\n","Processed 40960 sequences...\n","Processed 46080 sequences...\n","Processed 51200 sequences...\n","Processed 56320 sequences...\n","Processed 61440 sequences...\n","Processed 66560 sequences...\n","Processed 71680 sequences...\n","Processed 76800 sequences...\n","Processed 81920 sequences...\n","Processed 87040 sequences...\n","Processed 92160 sequences...\n","Processed 97280 sequences...\n","Processed 102400 sequences...\n","Processed 107520 sequences...\n","Processed 112640 sequences...\n","Processed 117760 sequences...\n","Processed 122880 sequences...\n","Processed 128000 sequences...\n","Processed 133120 sequences...\n","Processed 138240 sequences...\n","Processed 143360 sequences...\n","Processed 148480 sequences...\n","Processed 153600 sequences...\n","Processed 158720 sequences...\n","Processed 163840 sequences...\n","Processed 168960 sequences...\n","Processed 174080 sequences...\n","Processed 179200 sequences...\n","Processed 184320 sequences...\n","Processed 189440 sequences...\n","Processed 194560 sequences...\n","Processed 199680 sequences...\n","Processed 204800 sequences...\n","Processed 209920 sequences...\n","Processed 215040 sequences...\n","Processed 220160 sequences...\n","Processed 225280 sequences...\n","Processed 230400 sequences...\n","Processed 235520 sequences...\n","Processed 240640 sequences...\n","Processed 245760 sequences...\n","Processed 250880 sequences...\n","Processed 256000 sequences...\n","Processed 261120 sequences...\n","Processed 266240 sequences...\n","Processed 271360 sequences...\n","Processed 276480 sequences...\n","Processed 281600 sequences...\n","Processed 286720 sequences...\n","Processed 291840 sequences...\n","Processed 296960 sequences...\n","Processed 302080 sequences...\n","Processed 307200 sequences...\n","Processed 312320 sequences...\n","Processed 317440 sequences...\n","Processed 322560 sequences...\n","Computed embeddings shape: (323168, 256)\n","Creating complete embeddings array...\n","Final embeddings shape: (1000000, 256)\n","Final client IDs shape: (1000000,)\n","Clients with computed embeddings: 323168\n","Clients with zero embeddings: 676832\n","Total clients: 1000000\n","Saving complete embeddings and client IDs...\n","Complete embeddings generation finished!\n","Final shape: [1000000, 64]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0V5Q_kuK_EHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","length_counts = Counter(len(sublist) for sublist in train_sequences)\n","\n","sorted_counts = sorted(length_counts.items(), key=lambda x: x[0])\n","\n","for length, count in sorted_counts:\n","    print(f\"Length {length}: {count} times\")"],"metadata":{"id":"lZZBRIcXAQES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Step 1: Count ID occurrences\n","counts = Counter(id for sublist in train_sequences for id in sublist)\n","\n","# Step 2: Select IDs that appear >= 50 times\n","common_ids = {id for id, count in counts.items() if count >= 10}\n","\n","# Step 3: Filter original data to keep only frequent IDs\n","filtered_data = [\n","    [id for id in sublist if id in common_ids]\n","    for sublist in train_sequences\n","    if any(id in common_ids for id in sublist)\n","]\n","\n","# Step 4: Build mapping to dense IDs starting from 1\n","# Sorted for reproducibility (optional)\n","unique_ids = sorted(common_ids)\n","id_to_dense = {old_id: new_id for new_id, old_id in enumerate(unique_ids, start=1)}\n","\n","# Step 5: Apply mapping\n","dense_data = [\n","    [id_to_dense[id] for id in sublist]\n","    for sublist in filtered_data\n","]\n","\n","# Now `dense_data` contains only dense IDs from 1 to len(common_ids)\n","print(dense_data)\n"],"metadata":{"id":"sjeXF2f51KJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aLOQ8zAEJY7w"},"execution_count":null,"outputs":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":7366643,"sourceId":11734888,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[{"file_id":"1ioJaDyQDL3Af2NLYjkL_eOHKmWfO1_2a","timestamp":1748716833981}],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}