{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eab5ePbYx8g","executionInfo":{"status":"ok","timestamp":1748804126693,"user_tz":-120,"elapsed":22827,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"cd51a782-7625-40f0-9642-72c9333c6fdf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-05-08T15:31:25.592287Z","iopub.status.busy":"2025-05-08T15:31:25.592058Z","iopub.status.idle":"2025-05-08T15:31:29.564860Z","shell.execute_reply":"2025-05-08T15:31:29.564195Z","shell.execute_reply.started":"2025-05-08T15:31:25.592263Z"},"trusted":true,"id":"uWOYq4SvYsjI","executionInfo":{"status":"ok","timestamp":1748804130204,"user_tz":-120,"elapsed":3525,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["import random\n","import torch\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","\n","import pickle\n","import torch\n","import tqdm\n","from torch import optim\n","from torch.utils.data import DataLoader, random_split"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.567040Z","iopub.status.busy":"2025-05-08T15:31:29.566717Z","iopub.status.idle":"2025-05-08T15:31:29.626078Z","shell.execute_reply":"2025-05-08T15:31:29.624984Z","shell.execute_reply.started":"2025-05-08T15:31:29.567021Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"VI0xrN-zYsjL","executionInfo":{"status":"ok","timestamp":1748804130219,"user_tz":-120,"elapsed":11,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"64c09005-858d-447a-adb4-4a4554b80a95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Working on DEVICE=device(type='cuda')\n"]}],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Working on {DEVICE=}')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.627965Z","iopub.status.busy":"2025-05-08T15:31:29.627711Z","iopub.status.idle":"2025-05-08T15:31:29.649514Z","shell.execute_reply":"2025-05-08T15:31:29.648723Z","shell.execute_reply.started":"2025-05-08T15:31:29.627946Z"},"trusted":true,"id":"f05PzcL2YsjM","executionInfo":{"status":"ok","timestamp":1748804130245,"user_tz":-120,"elapsed":24,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from dataclasses import dataclass\n","import numpy as np\n","\n","\n","# GPT said this is a professional way to do it\n","@dataclass\n","class BERT4RecConfig:\n","    # EmbeddingLayer\n","    vocab_size: int\n","    embedding_dim: int\n","    max_seq_len: int\n","    embedding_dropout: float\n","\n","    # Encoder\n","    num_layers: int\n","    num_heads: int\n","    hidden_dim: int\n","    encoder_dropout: float\n","\n","    # ProjectionHead\n","    projection_dim: int\n","\n","\n","class EmbeddingLayer(nn.Module):\n","    \"\"\"Item + positional embeddings with layer normalization and dropout\"\"\"\n","\n","    def __init__(self, vocab_size: int, embedding_dim: int, max_seq_len: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.item_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.position_embeddings = nn.Embedding(max_seq_len, embedding_dim)\n","        self.layer_norm = nn.LayerNorm(embedding_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.vocab_size = vocab_size\n","\n","    def forward(self, x):\n","        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand_as(x)\n","        # print(x.max(), self.vocab_size, x.min())\n","        embeddings = self.item_embeddings(x) + self.position_embeddings(positions)\n","        embeddings = self.layer_norm(embeddings)\n","        return self.dropout(embeddings)\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"Transformer encoder. Wrapper for `torch.TransformerEncoderLayer`\"\"\"\n","\n","    def __init__(self, embedding_dim: int, num_layers: int, num_heads: int, hidden_dim: int,\n","                 dropout: float = 0.1) -> None:\n","        \"\"\"\n","\n","        Args:\n","            embedding_dim:\n","            num_layers:\n","            num_heads:\n","            hidden_dim:\n","            dropout:\n","\n","        Returns:\n","            None:\n","        \"\"\"\n","        super().__init__()\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embedding_dim,\n","            nhead=num_heads,\n","            dim_feedforward=hidden_dim,\n","            dropout=dropout,\n","            activation=\"gelu\",\n","            batch_first=True,\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","    def forward(self, x, src_key_padding_mask=None):\n","        # x: [batch_size, seq_len, embedding_dim]\n","        return self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n","\n","\n","class ProjectionHead(nn.Module):\n","    \"\"\"Projection head\"\"\"\n","\n","    def __init__(self, embedding_dim: int, projection_dim: int, vocab_size: int):\n","        super().__init__()\n","        # transform + layer normalization\n","        self.projection = nn.Linear(embedding_dim, projection_dim)\n","        self.activation = nn.GELU()\n","        self.layer_norm = nn.LayerNorm(projection_dim)\n","\n","        # map to final logits\n","        self.decoder = nn.Linear(projection_dim, vocab_size, bias=False)\n","        self.bias = nn.Parameter(torch.zeros(vocab_size))\n","\n","    def forward(self, x):\n","        # x: [batch_size, seq_len, embedding_dim]\n","        proj = self.projection(x)\n","        proj = self.activation(proj)\n","        proj = self.layer_norm(proj)\n","        return self.decoder(proj) + self.bias\n","\n","\n","class BERT4Rec(nn.Module):\n","    \"\"\"BERT4Rec model from `https://arxiv.org/pdf/1904.06690` paper\"\"\"\n","\n","    def __init__(self, config: BERT4RecConfig):\n","        super().__init__()\n","        self.embedding = EmbeddingLayer(\n","            vocab_size=config.vocab_size,\n","            embedding_dim=config.embedding_dim,\n","            max_seq_len=config.max_seq_len,\n","            dropout=config.embedding_dropout,\n","        )\n","        self.encoder = Encoder(\n","            embedding_dim=config.embedding_dim,\n","            num_layers=config.num_layers,\n","            num_heads=config.num_heads,\n","            hidden_dim=config.hidden_dim,\n","            dropout=config.encoder_dropout,\n","        )\n","        self.projection = ProjectionHead(\n","            embedding_dim=config.embedding_dim,\n","            projection_dim=config.projection_dim,\n","            vocab_size=config.vocab_size,\n","        )\n","\n","        # weights sharing\n","        self.projection.decoder.weight = self.embedding.item_embeddings.weight\n","\n","    def forward(self, x, mask=None):\n","        x = torch.clamp(x, 0, self.embedding.vocab_size - 1)\n","        pad_mask = x.eq(0)\n","        x = self.embedding(x)\n","        x = self.encoder(x, src_key_padding_mask=pad_mask)\n","        x = self.projection(x)\n","        return x"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.650594Z","iopub.status.busy":"2025-05-08T15:31:29.650327Z","iopub.status.idle":"2025-05-08T15:31:29.671986Z","shell.execute_reply":"2025-05-08T15:31:29.671217Z","shell.execute_reply.started":"2025-05-08T15:31:29.650572Z"},"trusted":true,"id":"GEgTk-FLYsjM","executionInfo":{"status":"ok","timestamp":1748804130296,"user_tz":-120,"elapsed":49,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["def masked_cross_entropy_loss(logits, labels, mask, eps=1e-8):\n","    logits = logits.view(-1, logits.size(-1))\n","    labels = labels.view(-1)\n","    mask = mask.view(-1).float()\n","\n","    loss = F.cross_entropy(logits, labels, reduction='none')\n","    masked_loss = loss * mask\n","    denom = mask.sum()\n","    return masked_loss.sum() / (denom + eps)\n","\n","\n","\n","def train_step(model, optimizer, batch, device=DEVICE):\n","    model.train()\n","    input_ids, labels, masked_pos = [x.to(device) for x in batch]  # all [B, L]\n","\n","    optimizer.zero_grad()\n","    logits = model(input_ids)  # [B, L, V]\n","    loss = masked_cross_entropy_loss(logits, labels, masked_pos)\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","\n","    return loss.item()\n","\n","\n","def validate_step(model, batch, top_k=10, device=DEVICE):\n","    model.eval()\n","    input_ids, labels, masked_pos = [x.to(device) for x in batch]\n","\n","    # Comprehensive validation\n","    vocab_size = model.embedding.item_embeddings.num_embeddings\n","\n","    if input_ids.max() >= vocab_size:\n","        print(f\"ERROR: Found index {input_ids.max().item()} >= vocab_size {vocab_size}\")\n","        print(f\"Problematic indices: {input_ids[input_ids >= vocab_size]}\")\n","        # Emergency fix: clamp to valid range\n","        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n","\n","    if input_ids.min() < 0:\n","        print(f\"ERROR: Found negative index {input_ids.min().item()}\")\n","        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n","\n","\n","    try:\n","        with torch.no_grad():\n","            logits = model(input_ids)\n","    except IndexError as e:\n","        print(f\"IndexError caught: {e}\")\n","        print(f\"Input stats: min={input_ids.min()}, max={input_ids.max()}, shape={input_ids.shape}\")\n","        return 0.0, 0.0, 0.0\n","\n","    # Rest of your validation logic...\n","    masked_logits = logits[masked_pos.bool()]\n","    masked_labels = labels[masked_pos.bool()]\n","\n","    if len(masked_logits) == 0:\n","        return 0.0, 0.0, 0.0\n","\n","    loss = F.cross_entropy(masked_logits, masked_labels, reduction='mean').item()\n","\n","    _, topk = masked_logits.topk(top_k, dim=-1)\n","    hits = (topk == masked_labels.unsqueeze(1)).float()\n","    hr = hits.any(dim=1).float().mean().item()\n","    ndcg = (hits / torch.log2(torch.arange(2, top_k + 2, device=hits.device).float())).sum(dim=1).mean().item()\n","\n","    return loss, hr, ndcg\n","\n","\n","\n","def mask_sequence(seq, mask_token_id, vocab_size, mask_prob=0.15, pad_token_id=0):\n","    \"\"\"\n","    Function takes a user sequence and randomly selects items to mask.\n","    Returns:\n","        - input_ids: modified sequence with [MASK] and others.\n","        - labels: target items only in masked positions (0 elsewhere).\n","        - masked_pos: binary mask of masked locations.\n","    \"\"\"\n","    input_ids = seq.copy()\n","    labels = [0] * len(seq)\n","    masked_pos = [0] * len(seq)\n","    # mask_prob = 0.5\n","\n","    for i in range(len(seq)):\n","        if seq[i] == pad_token_id:\n","            continue\n","        if random.random() < mask_prob:\n","            masked_pos[i] = 1\n","            labels[i] = seq[i]\n","            rand = random.random()\n","            if rand < 0.8:\n","                input_ids[i] = mask_token_id\n","            elif rand < 0.9:\n","                input_ids[i] = random.randint(1, vocab_size - 1)  # avoid pad token\n","            else:\n","                pass  # leave unchanged\n","    return input_ids, labels, masked_pos\n","\n","\n","def collate_fn(batch, mask_token_id, vocab_size, pad_token_id=0, max_len=None):\n","    \"\"\"\n","    Pads sequences to same length, converts everything to tensors.\n","    batch: list of sequences (list of item IDs)\n","    \"\"\"\n","    input_ids, labels, masked_pos = [], [], []\n","\n","    for seq in batch:\n","        if max_len is not None:\n","            seq = seq[-max_len:]  # truncate if needed\n","\n","        inp, lab, msk = mask_sequence(seq, mask_token_id, vocab_size, pad_token_id=pad_token_id)\n","        input_ids.append(torch.tensor(inp))\n","        labels.append(torch.tensor(lab))\n","        masked_pos.append(torch.tensor(msk))\n","\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n","    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n","    masked_pos = pad_sequence(masked_pos, batch_first=True, padding_value=0)\n","\n","    return input_ids, labels, masked_pos"]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"/content/drive/MyDrive/recsys/bert/full_data/pagevisit_bert_train.pkl\", \"rb\") as fh:\n","    train_sequences = pickle.load(fh)"],"metadata":{"id":"Ul9v2KdY7ACw","executionInfo":{"status":"ok","timestamp":1748804162534,"user_tz":-120,"elapsed":32235,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"from collections import Counter\n","\n","# Step 1: Count ID occurrences\n","counts = Counter(id for sublist in train_sequences for id in sublist)\n","\n","# Step 2: Select IDs that appear >= 50 times\n","common_ids = {id for id, count in counts.items() if count >= 10}\n","\n","# Step 3: Filter original data to keep only frequent IDs\n","filtered_data = [\n","    [id for id in sublist if id in common_ids]\n","    for sublist in train_sequences\n","    if any(id in common_ids for id in sublist)\n","]\n","\n","# Step 4: Build mapping to dense IDs starting from 1\n","# Sorted for reproducibility (optional)\n","unique_ids = sorted(common_ids)\n","id_to_dense = {old_id: new_id for new_id, old_id in enumerate(unique_ids, start=1)}\n","\n","# Step 5: Apply mapping\n","train_sequences = [\n","    [id_to_dense[id] for id in sublist]\n","    for sublist in filtered_data\n","]\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"qB6_jm666-0X","executionInfo":{"status":"ok","timestamp":1748804162573,"user_tz":-120,"elapsed":19,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"e97cd312-6227-445d-a3f8-b6f68b1faddc"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'from collections import Counter\\n\\n# Step 1: Count ID occurrences\\ncounts = Counter(id for sublist in train_sequences for id in sublist)\\n\\n# Step 2: Select IDs that appear >= 50 times\\ncommon_ids = {id for id, count in counts.items() if count >= 10}\\n\\n# Step 3: Filter original data to keep only frequent IDs\\nfiltered_data = [\\n    [id for id in sublist if id in common_ids]\\n    for sublist in train_sequences\\n    if any(id in common_ids for id in sublist)\\n]\\n\\n# Step 4: Build mapping to dense IDs starting from 1\\n# Sorted for reproducibility (optional)\\nunique_ids = sorted(common_ids)\\nid_to_dense = {old_id: new_id for new_id, old_id in enumerate(unique_ids, start=1)}\\n\\n# Step 5: Apply mapping\\ntrain_sequences = [\\n    [id_to_dense[id] for id in sublist]\\n    for sublist in filtered_data\\n]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":[],"metadata":{"id":"VnVBOfxs7LvW"}},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.673079Z","iopub.status.busy":"2025-05-08T15:31:29.672827Z","iopub.status.idle":"2025-05-08T15:31:29.689995Z","shell.execute_reply":"2025-05-08T15:31:29.689252Z","shell.execute_reply.started":"2025-05-08T15:31:29.673058Z"},"trusted":true,"id":"gyfjRMEbYsjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748804176180,"user_tz":-120,"elapsed":13603,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"0980e3ae-049d-4411-e296-d9c4da9767fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique items count: 1315819\n","Max item ID in sequences: 15869648\n","Min item ID in sequences: 1\n","VOCAB_SIZE: 1315821\n","MASK_ID: 1315820\n"]}],"source":["max_item_id = 1315819  # Since you start enumeration from 1\n","VOCAB_SIZE = max_item_id + 2   # +1 for PAD (0), +1 for MASK\n","MASK_ID = VOCAB_SIZE - 1\n","PAD_ID = 0\n","MAX_SEQUENCE_LEN = 128\n","\n","NUM_EPOCHS = 5\n","\n","print(f\"Unique items count: {1315819}\")\n","print(f\"Max item ID in sequences: {max(el for seq in train_sequences for el in seq)}\")\n","print(f\"Min item ID in sequences: {min(el for seq in train_sequences for el in seq)}\")\n","print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n","print(f\"MASK_ID: {MASK_ID}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.887068Z","iopub.status.busy":"2025-05-08T15:31:29.886851Z","iopub.status.idle":"2025-05-08T15:31:29.917228Z","shell.execute_reply":"2025-05-08T15:31:29.916395Z","shell.execute_reply.started":"2025-05-08T15:31:29.887051Z"},"trusted":true,"id":"yoH5rQe_YsjN","executionInfo":{"status":"ok","timestamp":1748804178220,"user_tz":-120,"elapsed":2037,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["train_size = int(0.9995 * len(train_sequences))\n","val_size = len(train_sequences) - train_size\n","\n","train_dataset, val_dataset = random_split(train_sequences, [train_size, val_size])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.918398Z","iopub.status.busy":"2025-05-08T15:31:29.918119Z","iopub.status.idle":"2025-05-08T15:31:29.924422Z","shell.execute_reply":"2025-05-08T15:31:29.923714Z","shell.execute_reply.started":"2025-05-08T15:31:29.918373Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"RvqIxaVyYsjN","executionInfo":{"status":"ok","timestamp":1748804178628,"user_tz":-120,"elapsed":403,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"dc8f292c-2573-4e51-aa4d-243bd5ad3965"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["72677"]},"metadata":{},"execution_count":10}],"source":["len(train_dataset) // 256"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:29.925608Z","iopub.status.busy":"2025-05-08T15:31:29.925253Z","iopub.status.idle":"2025-05-08T15:31:29.940596Z","shell.execute_reply":"2025-05-08T15:31:29.939682Z","shell.execute_reply.started":"2025-05-08T15:31:29.925584Z"},"trusted":true,"id":"rMlEZ3KsYsjO","executionInfo":{"status":"ok","timestamp":1748804178647,"user_tz":-120,"elapsed":4,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[],"source":["train_loader = DataLoader(\n","    dataset=train_dataset,\n","    batch_size=8,\n","    shuffle=True,\n","    collate_fn=lambda batch: collate_fn(batch, mask_token_id=MASK_ID, vocab_size=VOCAB_SIZE, max_len=MAX_SEQUENCE_LEN),\n",")\n","\n","val_loader = DataLoader(\n","    dataset=val_dataset,\n","    batch_size=16,\n","    shuffle=False,\n","    collate_fn=lambda batch: collate_fn(batch, mask_token_id=MASK_ID, vocab_size=VOCAB_SIZE, max_len=MAX_SEQUENCE_LEN),\n",")"]},{"cell_type":"code","source":["config = BERT4RecConfig(\n","    vocab_size=VOCAB_SIZE,\n","    embedding_dim=256,\n","    max_seq_len=MAX_SEQUENCE_LEN,\n","    embedding_dropout=0.2,\n","    num_layers=6,\n","    num_heads=8,\n","    hidden_dim=256,\n","    encoder_dropout=0.2,\n","    projection_dim=256,\n",")"],"metadata":{"id":"ABsHK2CKv8uI","executionInfo":{"status":"ok","timestamp":1748804178672,"user_tz":-120,"elapsed":12,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:30.319116Z","iopub.status.busy":"2025-05-08T15:31:30.318840Z","iopub.status.idle":"2025-05-08T15:31:34.590098Z","shell.execute_reply":"2025-05-08T15:31:34.589335Z","shell.execute_reply.started":"2025-05-08T15:31:30.319090Z"},"trusted":true,"id":"ttXFzdTmYsjO","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1748804178703,"user_tz":-120,"elapsed":27,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"38e544be-2ba2-4461-9e9f-67d9a3f8e96c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nmodel = BERT4Rec(config)\\nmodel = model.to(DEVICE)\\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["\"\"\"\n","model = BERT4Rec(config)\n","model = model.to(DEVICE)\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\"\"\""]},{"cell_type":"code","source":["\n","def evaluate(model, val_loader):\n","    model.eval()\n","    total_loss = 0.0\n","    total_hr10 = 0.0\n","    total_ndcg10 = 0.0\n","    total_batches = 0\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            val_loss, hr10, ndcg10 = validate_step(model, batch)\n","            total_loss += val_loss\n","            total_hr10 += hr10\n","            total_ndcg10 += ndcg10\n","            total_batches += 1\n","\n","    return (\n","        total_loss / total_batches,\n","        total_hr10 / total_batches,\n","        total_ndcg10 / total_batches\n","    )"],"metadata":{"id":"iQjkVmQ8wnFA","executionInfo":{"status":"ok","timestamp":1748804178726,"user_tz":-120,"elapsed":20,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-05-08T15:31:34.591237Z","iopub.status.busy":"2025-05-08T15:31:34.590870Z","iopub.status.idle":"2025-05-08T15:31:35.085293Z","shell.execute_reply":"2025-05-08T15:31:35.084189Z","shell.execute_reply.started":"2025-05-08T15:31:34.591211Z"},"trusted":true,"id":"KXEtWOrdYsjO","colab":{"base_uri":"https://localhost:8080/","height":199},"outputId":"7739ef3e-31d8-4865-ea61-29d09c68c84c","executionInfo":{"status":"ok","timestamp":1748804178753,"user_tz":-120,"elapsed":22,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'train_history = []\\nvalid_history = []\\n\\nNUM_EPOCHS = 5\\nfor epoch in range(NUM_EPOCHS):\\n    for step, batch in enumerate(tqdm.tqdm(train_loader, desc=\"Training\")):\\n        loss = train_step(model, optimizer, batch)\\n        train_history.append(loss)\\n        if step % 1000 == 0 and step != 0:\\n            print(f\"\\nstep {step} loss {np.mean(train_history[-1000:])}\")\\n        if step % 5000 == 0 and step != 0:\\n            val_loss, hr10, ndcg10 = validate_step(model, next(iter(val_loader)))\\n            valid_history.append((val_loss, hr10, ndcg10))\\n            print(f\"\\nEpoch {epoch + 1} | Val Loss: {val_loss:.4f} | HR@10: {hr10:.4f} | NDCG@10: {ndcg10:.4f}\")\\n            if val_loss == min(valid_history, key=lambda x: x[0])[0]:\\n              torch.save({\\n    \\'model_state_dict\\': model.state_dict(),\\n    \\'optimizer_state_dict\\': optimizer.state_dict(),\\n    \\'config\\': config\\n}, \\'/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_pagevisit.pt\\')\\n\\n    scheduler.step()'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["\"\"\"train_history = []\n","valid_history = []\n","\n","NUM_EPOCHS = 5\n","for epoch in range(NUM_EPOCHS):\n","    for step, batch in enumerate(tqdm.tqdm(train_loader, desc=\"Training\")):\n","        loss = train_step(model, optimizer, batch)\n","        train_history.append(loss)\n","        if step % 1000 == 0 and step != 0:\n","            print(f\"\\nstep {step} loss {np.mean(train_history[-1000:])}\")\n","        if step % 5000 == 0 and step != 0:\n","            val_loss, hr10, ndcg10 = validate_step(model, next(iter(val_loader)))\n","            valid_history.append((val_loss, hr10, ndcg10))\n","            print(f\"\\nEpoch {epoch + 1} | Val Loss: {val_loss:.4f} | HR@10: {hr10:.4f} | NDCG@10: {ndcg10:.4f}\")\n","            if val_loss == min(valid_history, key=lambda x: x[0])[0]:\n","              torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'config': config\n","}, '/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_pagevisit.pt')\n","\n","    scheduler.step()\"\"\""]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.status.busy":"2025-05-08T15:31:35.085950Z","iopub.status.idle":"2025-05-08T15:31:35.086173Z","shell.execute_reply":"2025-05-08T15:31:35.086081Z","shell.execute_reply.started":"2025-05-08T15:31:35.086071Z"},"trusted":true,"id":"yRJYor48YsjO","colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"status":"ok","timestamp":1748804178801,"user_tz":-120,"elapsed":29,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"de80bc91-96cf-4021-f13e-dc6b8b3dd7fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef moving_average(data, window_size):\\n    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\\n\\nwindow_size = 100\\ny = moving_average(train_history, window_size)\\nx = np.arange(len(y)) + window_size\\n\\nplt.plot(x, y, color='blue', alpha=0.5, label='train loss')\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\nplt.axhline(val_loss, color='black', linestyle='--', label='val loss')\\nplt.legend()\\nplt.grid(True)\\nplt.show()\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["\"\"\"import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def moving_average(data, window_size):\n","    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n","\n","window_size = 100\n","y = moving_average(train_history, window_size)\n","x = np.arange(len(y)) + window_size\n","\n","plt.plot(x, y, color='blue', alpha=0.5, label='train loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","plt.axhline(val_loss, color='black', linestyle='--', label='val loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\"\"\""]},{"cell_type":"code","execution_count":17,"metadata":{"id":"c1_C4sogYsjO","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1748804178821,"user_tz":-120,"elapsed":15,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"dd734d1c-8880-41bd-a5f3-0667501e2a8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"torch.save({\\n    'model_state_dict': model.state_dict(),\\n    'optimizer_state_dict': optimizer.state_dict(),\\n    'config': config\\n}, '/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_pagevisit.pt')\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["\"\"\"torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'config': config\n","}, '/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_pagevisit.pt')\"\"\"\n"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pickle\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","# Load the saved model\n","print(\"Loading model...\")\n","checkpoint = torch.load('/content/drive/MyDrive/recsys/bert/full_data/bert4rec_model_512dim_bigger_30seqlen_pagevisit.pt', weights_only=False)\n","\n","# Initialize model with saved config\n","model = BERT4Rec(checkpoint['config'])\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Move to device and set to eval mode\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(DEVICE)\n","model.eval()\n","\n","print(\"Model loaded successfully!\")\n","\n","# Get configuration\n","PAD = 0\n","max_len = MAX_SEQUENCE_LEN\n","\n","# Load and filter data\n","print(\"Loading training data...\")\n","with open(\"/content/drive/MyDrive/recsys/bert/full_data/pagevisit_bert_train.pkl\", \"rb\") as fh:\n","    train_sequences = pickle.load(fh)\n","\n","with open(\"/content/drive/MyDrive/recsys/bert/full_data/pagevisit_clients_train.pkl\", \"rb\") as fh:\n","    clients = pickle.load(fh)\n","\n","from collections import Counter\n","\n","# Step 1: Count ID occurrences\n","counts = Counter(id for sublist in train_sequences for id in sublist)\n","\n","# Step 2: Select IDs that appear >= 50 times\n","common_ids = {id for id, count in counts.items() if count >= 10}\n","\n","# Step 3: Filter original data to keep only frequent IDs\n","filtered_data = [\n","    [id for id in sublist if id in common_ids]\n","    for sublist in train_sequences\n","]\n","\n","# Step 4: Build mapping to dense IDs starting from 1\n","# Sorted for reproducibility (optional)\n","unique_ids = sorted(common_ids)\n","id_to_dense = {old_id: new_id for new_id, old_id in enumerate(unique_ids, start=1)}\n","\n","# Step 5: Apply mapping\n","train_sequences = [\n","    [id_to_dense[id] for id in sublist]\n","    for sublist in filtered_data\n","]\n","\n","relevant_clients = np.load(\"/content/drive/MyDrive/recsys/original_data/input/relevant_clients.npy\")\n","\n","print(f\"Total sequences: {len(train_sequences)}\")\n","print(f\"Relevant clients: {len(relevant_clients)}\")\n","\n","# Convert to sets for efficient lookup\n","relevant_client_set = set(relevant_clients)\n","available_client_set = set(clients)\n","\n","# Find clients that are in relevant_clients but not in available clients\n","missing_clients = relevant_client_set - available_client_set\n","print(f\"Missing clients (in relevant but not in training): {len(missing_clients)}\")\n","\n","# Filter sequences for relevant clients that exist in training data\n","filtered_data = []\n","filtered_client_ids = []\n","\n","for i, client in enumerate(clients):\n","    if client in relevant_client_set:\n","        filtered_data.append(train_sequences[i])\n","        filtered_client_ids.append(client)\n","\n","print(f\"Filtered sequences with data: {len(filtered_data)}\")\n","\n","# Pad sequences and create attention masks\n","def pad_sequence_to_length(sequence, target_length, pad_value=PAD):\n","    if len(sequence) >= target_length:\n","        return sequence[:target_length]\n","    else:\n","        return sequence + [pad_value] * (target_length - len(sequence))\n","\n","padded_sequences = []\n","attention_masks = []\n","\n","for seq in filtered_data:\n","    padded_seq = pad_sequence_to_length(seq, max_len, PAD)\n","    mask = [1 if token != PAD else 0 for token in padded_seq]\n","    padded_sequences.append(padded_seq)\n","    attention_masks.append(mask)\n","\n","# Generate embeddings for clients with data\n","print(\"Generating embeddings for clients with data...\")\n","batch_size = 512\n","computed_embeddings = []\n","\n","if len(filtered_data) > 0:\n","    padded_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n","    mask_tensor = torch.tensor(attention_masks, dtype=torch.float)\n","    dataset = TensorDataset(padded_tensor, mask_tensor)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    def mean_pooling(embeddings, attention_mask):\n","        \"\"\"Apply mean pooling with attention mask to ignore padding tokens\"\"\"\n","        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size())\n","        sum_embeddings = torch.sum(embeddings * mask_expanded, dim=1)\n","        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (batch_sequences, batch_masks) in enumerate(dataloader):\n","            batch_sequences = batch_sequences.to(DEVICE)\n","            batch_masks = batch_masks.to(DEVICE)\n","\n","            embeddings = model.embedding(batch_sequences)\n","            embeddings = model.encoder(embeddings)\n","\n","            pooled_embeddings = mean_pooling(embeddings, batch_masks)\n","            pooled_embeddings = pooled_embeddings.cpu().numpy().astype(np.float16)\n","            computed_embeddings.append(pooled_embeddings)\n","\n","            if (batch_idx + 1) % 10 == 0:\n","                print(f\"Processed {(batch_idx + 1) * batch_size} sequences...\")\n","\n","    computed_embeddings = np.concatenate(computed_embeddings, axis=0)\n","else:\n","    computed_embeddings = np.empty((0, 64), dtype=np.float16)\n","\n","print(f\"Computed embeddings shape: {computed_embeddings.shape}\")\n","\n","# Create complete embeddings array for all relevant clients\n","print(\"Creating complete embeddings array...\")\n","\n","# Sort relevant clients for consistent ordering\n","sorted_relevant_clients = np.sort(relevant_clients)\n","\n","# Create mapping from client_id to embedding\n","client_to_embedding = {}\n","for i, client_id in enumerate(filtered_client_ids):\n","    client_to_embedding[client_id] = computed_embeddings[i]\n","\n","# Create final arrays\n","final_embeddings = []\n","final_client_ids = []\n","\n","for client_id in sorted_relevant_clients:\n","    final_client_ids.append(client_id)\n","\n","    if client_id in client_to_embedding:\n","        # Use computed embedding\n","        final_embeddings.append(client_to_embedding[client_id])\n","    else:\n","        # Use zero embedding for missing clients\n","        zero_embedding = np.zeros(256, dtype=np.float16)\n","        final_embeddings.append(zero_embedding)\n","\n","# Convert to numpy arrays\n","final_embeddings = np.array(final_embeddings, dtype=np.float16)\n","final_client_ids = np.array(final_client_ids, dtype=np.int64)\n","\n","print(f\"Final embeddings shape: {final_embeddings.shape}\")\n","print(f\"Final client IDs shape: {final_client_ids.shape}\")\n","print(f\"Clients with computed embeddings: {len(filtered_client_ids)}\")\n","print(f\"Clients with zero embeddings: {len(missing_clients)}\")\n","print(f\"Total clients: {len(final_client_ids)}\")\n","\n","# Verify all relevant clients are included\n","assert len(final_client_ids) == len(relevant_clients), \"Mismatch in client count!\"\n","assert set(final_client_ids) == set(relevant_clients), \"Client sets don't match!\"\n","\n","# Save the results\n","print(\"Saving complete embeddings and client IDs...\")\n","np.save('/content/drive/MyDrive/recsys/bert/full_data/embeddings_512dim_bigger_30seqlen_pagevisit.npy', final_embeddings)\n","np.save('/content/drive/MyDrive/recsys/bert/full_data/client_ids_512dim_bigger_30seqlen_pagevisit.npy', final_client_ids)\n","\n","print(\"Complete embeddings generation finished!\")\n","print(f\"Final shape: [{len(final_client_ids)}, {64}]\")\n"],"metadata":{"id":"k0-KBdjDEY7X","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1748804900755,"user_tz":-120,"elapsed":298902,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}},"outputId":"a74cfab5-17d1-4cb4-affb-95bf8a5957dd"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Model loaded successfully!\n","Loading training data...\n","Total sequences: 18614702\n","Relevant clients: 1000000\n","Missing clients (in relevant but not in training): 155077\n","Filtered sequences with data: 844923\n","Generating embeddings for clients with data...\n","Processed 5120 sequences...\n","Processed 10240 sequences...\n","Processed 15360 sequences...\n","Processed 20480 sequences...\n","Processed 25600 sequences...\n","Processed 30720 sequences...\n","Processed 35840 sequences...\n","Processed 40960 sequences...\n","Processed 46080 sequences...\n","Processed 51200 sequences...\n","Processed 56320 sequences...\n","Processed 61440 sequences...\n","Processed 66560 sequences...\n","Processed 71680 sequences...\n","Processed 76800 sequences...\n","Processed 81920 sequences...\n","Processed 87040 sequences...\n","Processed 92160 sequences...\n","Processed 97280 sequences...\n","Processed 102400 sequences...\n","Processed 107520 sequences...\n","Processed 112640 sequences...\n","Processed 117760 sequences...\n","Processed 122880 sequences...\n","Processed 128000 sequences...\n","Processed 133120 sequences...\n","Processed 138240 sequences...\n","Processed 143360 sequences...\n","Processed 148480 sequences...\n","Processed 153600 sequences...\n","Processed 158720 sequences...\n","Processed 163840 sequences...\n","Processed 168960 sequences...\n","Processed 174080 sequences...\n","Processed 179200 sequences...\n","Processed 184320 sequences...\n","Processed 189440 sequences...\n","Processed 194560 sequences...\n","Processed 199680 sequences...\n","Processed 204800 sequences...\n","Processed 209920 sequences...\n","Processed 215040 sequences...\n","Processed 220160 sequences...\n","Processed 225280 sequences...\n","Processed 230400 sequences...\n","Processed 235520 sequences...\n","Processed 240640 sequences...\n","Processed 245760 sequences...\n","Processed 250880 sequences...\n","Processed 256000 sequences...\n","Processed 261120 sequences...\n","Processed 266240 sequences...\n","Processed 271360 sequences...\n","Processed 276480 sequences...\n","Processed 281600 sequences...\n","Processed 286720 sequences...\n","Processed 291840 sequences...\n","Processed 296960 sequences...\n","Processed 302080 sequences...\n","Processed 307200 sequences...\n","Processed 312320 sequences...\n","Processed 317440 sequences...\n","Processed 322560 sequences...\n","Processed 327680 sequences...\n","Processed 332800 sequences...\n","Processed 337920 sequences...\n","Processed 343040 sequences...\n","Processed 348160 sequences...\n","Processed 353280 sequences...\n","Processed 358400 sequences...\n","Processed 363520 sequences...\n","Processed 368640 sequences...\n","Processed 373760 sequences...\n","Processed 378880 sequences...\n","Processed 384000 sequences...\n","Processed 389120 sequences...\n","Processed 394240 sequences...\n","Processed 399360 sequences...\n","Processed 404480 sequences...\n","Processed 409600 sequences...\n","Processed 414720 sequences...\n","Processed 419840 sequences...\n","Processed 424960 sequences...\n","Processed 430080 sequences...\n","Processed 435200 sequences...\n","Processed 440320 sequences...\n","Processed 445440 sequences...\n","Processed 450560 sequences...\n","Processed 455680 sequences...\n","Processed 460800 sequences...\n","Processed 465920 sequences...\n","Processed 471040 sequences...\n","Processed 476160 sequences...\n","Processed 481280 sequences...\n","Processed 486400 sequences...\n","Processed 491520 sequences...\n","Processed 496640 sequences...\n","Processed 501760 sequences...\n","Processed 506880 sequences...\n","Processed 512000 sequences...\n","Processed 517120 sequences...\n","Processed 522240 sequences...\n","Processed 527360 sequences...\n","Processed 532480 sequences...\n","Processed 537600 sequences...\n","Processed 542720 sequences...\n","Processed 547840 sequences...\n","Processed 552960 sequences...\n","Processed 558080 sequences...\n","Processed 563200 sequences...\n","Processed 568320 sequences...\n","Processed 573440 sequences...\n","Processed 578560 sequences...\n","Processed 583680 sequences...\n","Processed 588800 sequences...\n","Processed 593920 sequences...\n","Processed 599040 sequences...\n","Processed 604160 sequences...\n","Processed 609280 sequences...\n","Processed 614400 sequences...\n","Processed 619520 sequences...\n","Processed 624640 sequences...\n","Processed 629760 sequences...\n","Processed 634880 sequences...\n","Processed 640000 sequences...\n","Processed 645120 sequences...\n","Processed 650240 sequences...\n","Processed 655360 sequences...\n","Processed 660480 sequences...\n","Processed 665600 sequences...\n","Processed 670720 sequences...\n","Processed 675840 sequences...\n","Processed 680960 sequences...\n","Processed 686080 sequences...\n","Processed 691200 sequences...\n","Processed 696320 sequences...\n","Processed 701440 sequences...\n","Processed 706560 sequences...\n","Processed 711680 sequences...\n","Processed 716800 sequences...\n","Processed 721920 sequences...\n","Processed 727040 sequences...\n","Processed 732160 sequences...\n","Processed 737280 sequences...\n","Processed 742400 sequences...\n","Processed 747520 sequences...\n","Processed 752640 sequences...\n","Processed 757760 sequences...\n","Processed 762880 sequences...\n","Processed 768000 sequences...\n","Processed 773120 sequences...\n","Processed 778240 sequences...\n","Processed 783360 sequences...\n","Processed 788480 sequences...\n","Processed 793600 sequences...\n","Processed 798720 sequences...\n","Processed 803840 sequences...\n","Processed 808960 sequences...\n","Processed 814080 sequences...\n","Processed 819200 sequences...\n","Processed 824320 sequences...\n","Processed 829440 sequences...\n","Processed 834560 sequences...\n","Processed 839680 sequences...\n","Processed 844800 sequences...\n","Computed embeddings shape: (844923, 256)\n","Creating complete embeddings array...\n"]},{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1000000,) + inhomogeneous part.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-252acc3b75e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# Convert to numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mfinal_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mfinal_client_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_client_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1000000,) + inhomogeneous part."]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","length_counts = Counter(len(sublist) for sublist in train_sequences)\n","\n","sorted_counts = sorted(length_counts.items(), key=lambda x: x[0])\n","\n","for length, count in sorted_counts:\n","    print(f\"Length {length}: {count} times\")"],"metadata":{"id":"lZZBRIcXAQES","executionInfo":{"status":"aborted","timestamp":1748804415815,"user_tz":-120,"elapsed":312252,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Step 1: Count ID occurrences\n","counts = Counter(id for sublist in train_sequences for id in sublist)\n","\n","# Step 2: Select IDs that appear >= 50 times\n","common_ids = {id for id, count in counts.items() if count >= 10}\n","\n","# Step 3: Filter original data to keep only frequent IDs\n","filtered_data = [\n","    [id for id in sublist if id in common_ids]\n","    for sublist in train_sequences\n","    if any(id in common_ids for id in sublist)\n","]\n","\n","# Step 4: Build mapping to dense IDs starting from 1\n","# Sorted for reproducibility (optional)\n","unique_ids = sorted(common_ids)\n","id_to_dense = {old_id: new_id for new_id, old_id in enumerate(unique_ids, start=1)}\n","\n","# Step 5: Apply mapping\n","dense_data = [\n","    [id_to_dense[id] for id in sublist]\n","    for sublist in filtered_data\n","]\n","\n","# Now `dense_data` contains only dense IDs from 1 to len(common_ids)\n","print(dense_data)\n"],"metadata":{"id":"sjeXF2f51KJk","executionInfo":{"status":"aborted","timestamp":1748804415838,"user_tz":-120,"elapsed":312273,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aLOQ8zAEJY7w","executionInfo":{"status":"aborted","timestamp":1748804415842,"user_tz":-120,"elapsed":312275,"user":{"displayName":"Adam Stajek","userId":"09534021670324781377"}}},"execution_count":null,"outputs":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":7366643,"sourceId":11734888,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[{"file_id":"1ioJaDyQDL3Af2NLYjkL_eOHKmWfO1_2a","timestamp":1748716833981}],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}